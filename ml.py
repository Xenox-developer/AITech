import os
import json
import logging
import re
from typing import List, Dict, Tuple, Any
import numpy as np
from pathlib import Path
from dotenv import load_dotenv
from datetime import datetime, timedelta

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
load_dotenv()

# –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF
from pdfminer.high_level import extract_text

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ/–∞—É–¥–∏–æ
import whisperx
import torch

# NLP
from sentence_transformers import SentenceTransformer
from sklearn.cluster import HDBSCAN
from sklearn.preprocessing import normalize
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from collections import Counter

from openai import OpenAI

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
device = "cuda" if torch.cuda.is_available() else "cpu"
compute_type = "float16" if device == "cuda" else "int8"

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π
sentence_model = None
whisper_model = None
openai_client = None

def load_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π"""
    global sentence_model, whisper_model, openai_client
    
    logger.info("Loading models...")
    
    # Sentence transformer
    sentence_model = SentenceTransformer("intfloat/e5-large-v2", device=device)
    
    # Whisper –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ
    try:
        whisper_model = whisperx.load_model("large-v3", device, compute_type=compute_type)
    except Exception as e:
        logger.warning(f"Whisper model not loaded: {str(e)}")
    
    # OpenAI –∫–ª–∏–µ–Ω—Ç
    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        raise ValueError("OPENAI_API_KEY environment variable not set")
    openai_client = OpenAI(api_key=api_key)
    
    logger.info("Models loaded successfully")

try:
    load_models()
except Exception as e:
    logger.warning(f"Models not loaded on import: {str(e)}")

def extract_text_from_pdf(filepath: str) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
    try:
        text = extract_text(filepath)
        return text.strip()
    except Exception as e:
        logger.error(f"Error extracting text from PDF: {str(e)}")
        raise

def parse_page_range(page_range: str, max_pages: int = None) -> List[int]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü"""
    if not page_range or page_range.lower() in ['–≤—Å–µ', 'all']:
        if max_pages:
            return list(range(1, min(max_pages + 1, 21)))  # –ú–∞–∫—Å–∏–º—É–º 20 —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return []
    
    pages = set()
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º
    ranges = [r.strip() for r in page_range.split(',')]
    
    for range_str in ranges:
        if '-' in range_str:
            # –î–∏–∞–ø–∞–∑–æ–Ω —Å—Ç—Ä–∞–Ω–∏—Ü
            try:
                start, end = map(int, range_str.split('-'))
                if start > end:
                    start, end = end, start
                pages.update(range(start, end + 1))
            except ValueError:
                logger.warning(f"Invalid page range: {range_str}")
                continue
        else:
            # –û–¥–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞
            try:
                page = int(range_str)
                pages.add(page)
            except ValueError:
                logger.warning(f"Invalid page number: {range_str}")
                continue
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü
    if max_pages:
        pages = {p for p in pages if 1 <= p <= max_pages}
    
    return sorted(list(pages))

def extract_text_from_pdf_with_pages(filepath: str, page_range: str = None) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Å –≤—ã–±–æ—Ä–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü"""
    try:
        from pdfminer.high_level import extract_text_to_fp
        from pdfminer.layout import LAParams
        from pdfminer.pdfpage import PDFPage
        from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
        from pdfminer.converter import TextConverter
        import io
        
        if not page_range:
            # –ï—Å–ª–∏ –¥–∏–∞–ø–∞–∑–æ–Ω –Ω–µ —É–∫–∞–∑–∞–Ω, –∏–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º)
            text = extract_text(filepath)
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 20 —Å—Ç—Ä–∞–Ω–∏—Ü)
            max_chars = 128000
            if len(text) > max_chars:
                text = text[:max_chars] + "\n\n[–¢–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏]"
                logger.info(f"Text truncated to {max_chars} characters")
            return text.strip()
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
        with open(filepath, 'rb') as file:
            pages_count = len(list(PDFPage.get_pages(file)))
        
        logger.info(f"PDF has {pages_count} pages")
        
        # –ü–∞—Ä—Å–∏–º –¥–∏–∞–ø–∞–∑–æ–Ω —Å—Ç—Ä–∞–Ω–∏—Ü
        pages_to_extract = parse_page_range(page_range, pages_count)
        
        if not pages_to_extract:
            # –ï—Å–ª–∏ –¥–∏–∞–ø–∞–∑–æ–Ω –ø—É—Å—Ç–æ–π, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 20 —Å—Ç—Ä–∞–Ω–∏—Ü
            pages_to_extract = list(range(1, min(pages_count + 1, 21)))
        
        logger.info(f"Extracting pages: {pages_to_extract}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        output_string = io.StringIO()
        with open(filepath, 'rb') as file:
            rsrcmgr = PDFResourceManager()
            device = TextConverter(rsrcmgr, output_string, laparams=LAParams())
            interpreter = PDFPageInterpreter(rsrcmgr, device)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ 0-based –∏–Ω–¥–µ–∫—Å—ã
            page_indices = {p - 1 for p in pages_to_extract}
            
            for page_num, page in enumerate(PDFPage.get_pages(file)):
                if page_num in page_indices:
                    interpreter.process_page(page)
            
            device.close()
        
        text = output_string.getvalue()
        output_string.close()
        
        if not text.strip():
            logger.warning("No text extracted from specified pages, falling back to full extraction")
            return extract_text(filepath).strip()
        
        logger.info(f"Extracted {len(text)} characters from {len(pages_to_extract)} pages")
        return text.strip()
        
    except Exception as e:
        logger.error(f"Error extracting text from PDF pages: {str(e)}")
        # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—é
        logger.info("Falling back to full PDF extraction")
        return extract_text(filepath).strip()

def transcribe_video_with_timestamps(filepath: str) -> Dict[str, Any]:
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≤–∏–¥–µ–æ/–∞—É–¥–∏–æ"""
    try:
        logger.info(f"Transcribing video with timestamps: {filepath}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ
        audio = whisperx.load_audio(filepath)
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –æ—Ç–º–µ—Ç–∫–∞–º–∏
        result = whisper_model.transcribe(audio, batch_size=16)
        
        # –°–µ–≥–º–µ–Ω—Ç—ã –ø—Ä–æ—Ü–µ—Å—Å–∞
        segments = []
        key_moments = []
        full_text = ""
        
        for i, segment in enumerate(result["segments"]):
            text = segment["text"].strip()
            start = segment["start"]
            end = segment["end"]
            
            full_text += text + " "
            
            # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–∞ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤)
            importance = min(1.0, len(text.split()) / 50)
            
            segments.append({
                "start": start,
                "end": end,
                "text": text,
                "importance": importance
            })
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ (–±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é)
            if importance > 0.7 and len(text.split()) > 20:
                key_moments.append({
                    "time": start,
                    "description": text[:100] + "..." if len(text) > 100 else text
                })
        
        return {
            "full_text": full_text.strip(),
            "segments": segments,
            "key_moments": key_moments[:10]  # –¢–æ–ø 10 –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤
        }
        
    except Exception as e:
        logger.error(f"Error transcribing video: {str(e)}")
        return {"full_text": transcribe_video_simple(filepath), "segments": [], "key_moments": []}

def transcribe_video_simple(filepath: str) -> str:
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≤–∏–¥–µ–æ –±–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫"""
    try:
        audio = whisperx.load_audio(filepath)
        result = whisper_model.transcribe(audio, batch_size=16)
        text = " ".join([segment["text"] for segment in result["segments"]])
        return text.strip()
    except Exception as e:
        logger.error(f"Error in simple transcription: {str(e)}")
        raise

def extract_topics_with_gpt(text: str) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏–∫ —Å GPT"""
    try:
        if not openai_client:
            load_models()
        
        # –õ–∏–º–∏—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è API
        max_chars = 128000
        if len(text) > max_chars:
            text = text[:max_chars] + "..."
        
        prompt = """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —É—á–µ–±–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –∏–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

–í–µ—Ä–Ω–∏ JSON –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ:
{
  "main_topics": [
    {
      "title": "–ö—Ä–∞—Ç–∫–æ–µ, –ø–æ–Ω—è—Ç–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã (–Ω–µ –∫–æ–ø–∏—è —Ç–µ–∫—Å—Ç–∞!)",
      "summary": "–û–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ç–µ–º—ã —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
      "subtopics": ["–ü–æ–¥—Ç–µ–º–∞ 1", "–ü–æ–¥—Ç–µ–º–∞ 2"],
      "key_concepts": ["–ö–ª—é—á–µ–≤–æ–µ –ø–æ–Ω—è—Ç–∏–µ 1", "–ö–ª—é—á–µ–≤–æ–µ –ø–æ–Ω—è—Ç–∏–µ 2"],
      "complexity": "basic/intermediate/advanced",
      "examples": ["–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è"],
      "why_important": "–ü–æ—á–µ–º—É —ç—Ç–∞ —Ç–µ–º–∞ –≤–∞–∂–Ω–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞"
    }
  ],
  "concept_map": {
    "relationships": [
      {
        "from": "–¢–µ–º–∞ 1",
        "to": "–¢–µ–º–∞ 2", 
        "type": "causes/requires/similar/contrast/part_of",
        "description": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–≤—è–∑–∏"
      }
    ]
  },
  "learning_objectives": ["–ß—Ç–æ —Å—Ç—É–¥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –ø–æ–Ω—è—Ç—å –ø–æ—Å–ª–µ –∏–∑—É—á–µ–Ω–∏—è"],
  "prerequisites": ["–ß—Ç–æ –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å –¥–æ –∏–∑—É—á–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞"]
}

–í–ê–ñ–ù–û:
1. –ù–ï –∫–æ–ø–∏—Ä—É–π —Ç–µ–∫—Å—Ç –¥–æ—Å–ª–æ–≤–Ω–æ! –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
2. –°–æ–∑–¥–∞–π –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏", –∞ –Ω–µ "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è...")
3. –ò–∑–≤–ª–µ–∫–∏ 5-10 –≥–ª–∞–≤–Ω—ã—Ö —Ç–µ–º
4. –ö–∞–∂–¥–∞—è —Ç–µ–º–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏—á–µ—Å–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–π
5. –ü—Ä–∏–º–µ—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º–∏

–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:"""
        
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. –ò–∑–≤–ª–µ–∫–∞–π –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ç–µ–º—ã, –∞ –Ω–µ –∫–æ–ø–∏—Ä—É–π —Ç–µ–∫—Å—Ç."},
                {"role": "user", "content": f"{prompt}\n\n{text}"}
            ],
            temperature=0.3,
            max_tokens=3000,
            response_format={"type": "json_object"}
        )
        
        topics_data = json.loads(response.choices[0].message.content)
        
        # Ensure all required fields
        for topic in topics_data.get("main_topics", []):
            topic.setdefault("why_important", "–í–∞–∂–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞")
            topic.setdefault("examples", [])
            topic.setdefault("subtopics", [])
            topic.setdefault("key_concepts", [])
        
        return topics_data
        
    except Exception as e:
        logger.error(f"Error extracting topics with GPT: {str(e)}")
        # Fallback to old method
        return extract_topics_fallback(text)

def extract_topics_fallback(text: str) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏–∫ –±–µ–∑ GPT"""
    try:
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã –≤–º–µ—Å—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        paragraphs = [p.strip() for p in text.split('\n\n') if len(p.strip()) > 50]
        
        if len(paragraphs) < 3:
            # –ï—Å–ª–∏ –∞–±–∑–∞—Ü–µ–≤ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Ä–∞–∑–±–∏–≤–∞–µ–º –∏—Ö –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = sent_tokenize(text)
            paragraphs = []
            for i in range(0, len(sentences), 3):
                paragraph = " ".join(sentences[i:i+3])
                if len(paragraph) > 50:
                    paragraphs.append(paragraph)
        
        if len(paragraphs) < 2:
            return {
                "main_topics": [{
                    "title": "–û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                    "summary": "–î–æ–∫—É–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞",
                    "subtopics": [],
                    "key_concepts": [],
                    "complexity": "basic",
                    "examples": [],
                    "why_important": "–û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"
                }],
                "concept_map": {"relationships": []},
                "learning_objectives": ["–ò–∑—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"],
                "prerequisites": []
            }
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
        embeddings = sentence_model.encode(paragraphs, convert_to_tensor=False)
        embeddings = normalize(embeddings)
        
        # –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        min_cluster_size = max(2, min(5, len(paragraphs) // 5))
        clusterer = HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=1,
            metric='euclidean',
            cluster_selection_method='eom'
        )
        
        cluster_labels = clusterer.fit_predict(embeddings)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏–∫ –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        main_topics = []
        unique_labels = set(cluster_labels)
        
        for label in unique_labels:
            if label == -1:
                continue
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–±–∑–∞—Ü–µ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
            cluster_paragraphs = [paragraphs[i] for i, l in enumerate(cluster_labels) if l == label]
            
            if not cluster_paragraphs:
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–Ω–µ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ).
            title = extract_topic_title(cluster_paragraphs)
            
            # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
            summary = create_topic_summary(cluster_paragraphs)
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
            key_concepts = extract_key_concepts(" ".join(cluster_paragraphs))
            
            # –°–ª–æ–∂–Ω–æ—Å—Ç—å
            complexity = determine_complexity(" ".join(cluster_paragraphs))
            
            # –ü—Ä–∏–º–µ—Ä—ã
            examples = extract_meaningful_examples(cluster_paragraphs)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–ø–∏–∫–∞
            topic = {
                "title": title,
                "summary": summary,
                "subtopics": extract_subtopics_smart(cluster_paragraphs),
                "key_concepts": key_concepts[:5],
                "complexity": complexity,
                "examples": examples[:3],
                "why_important": f"–≠—Ç–∞ —Ç–µ–º–∞ –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å {title.lower()}"
            }
            
            main_topics.append(topic)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        main_topics.sort(key=lambda x: len(x["summary"]), reverse=True)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏
        relationships = extract_smart_relationships(main_topics, text)
        
        # –í—ã–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º –æ–±—É—á–µ–Ω–∏—è
        learning_objectives = extract_learning_objectives(main_topics)
        
        return {
            "main_topics": main_topics[:10],
            "concept_map": {"relationships": relationships},
            "learning_objectives": learning_objectives,
            "prerequisites": []
        }
        
    except Exception as e:
        logger.error(f"Error in fallback topic extraction: {str(e)}")
        return {
            "main_topics": [{
                "title": "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞",
                "summary": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–º—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "subtopics": [],
                "key_concepts": [],
                "complexity": "basic",
                "examples": [],
                "why_important": "–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–æ–π –∞–Ω–∞–ª–∏–∑"
            }],
            "concept_map": {"relationships": []},
            "learning_objectives": ["–ò–∑—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç"],
            "prerequisites": []
        }

def extract_topic_title(paragraphs: List[str]) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π —Ç–µ–º–∞—Ç–∏–∫ –∏–∑ –∞–±–∑–∞—Ü–µ–≤"""
    # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–ª–∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
    for para in paragraphs:
        lines = para.split('\n')
        for line in lines:
            if len(line) < 100 and (
                line.strip().startswith('¬ß') or 
                line.strip()[0].isdigit() or
                line.isupper() or
                ':' in line[:50]
            ):
                title = line.strip()
                title = re.sub(r'^[¬ß\d\s.]+', '', title)
                title = title.strip(':').strip()
                if len(title) > 10:
                    return title[:80]
    
    # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–º, –∏–∑–≤–ª–µ–∫–∞–µ–º —Ñ—Ä–∞–∑—É
    all_text = " ".join(paragraphs[:2])
    
    # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
    concept_patterns = [
        r'([–ê-–Ø][–∞-—è]+(?:\s+[–∞-—è]+){0,3})\s+(?:‚Äî|—ç—Ç–æ|—è–≤–ª—è–µ—Ç—Å—è|–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç)',
        r'(?:–†–∞—Å—Å–º–æ—Ç—Ä–∏–º|–ò–∑—É—á–∏–º|–û–±—Å—É–¥–∏–º)\s+([–∞-—è]+(?:\s+[–∞-—è]+){0,3})',
        r'(?:–∑–∞–¥–∞—á[–∞–∏]|–º–µ—Ç–æ–¥|–∞–ª–≥–æ—Ä–∏—Ç–º|—Ñ—É–Ω–∫—Ü–∏[—è–∏]|–º–æ–¥–µ–ª[—å–∏])\s+([–∞-—è]+(?:\s+[–∞-—è]+){0,2})'
    ]
    
    for pattern in concept_patterns:
        match = re.search(pattern, all_text, re.IGNORECASE)
        if match:
            title = match.group(1).strip()
            if len(title) > 10:
                return title.capitalize()
    
    words = word_tokenize(all_text.lower())
    important_words = [w for w in words if len(w) > 4 and w.isalpha()]
    word_freq = Counter(important_words)
    
    if word_freq:
        top_words = [word for word, _ in word_freq.most_common(3)]
        return " ".join(top_words).capitalize()
    
    return "–¢–µ–º–∞ —Ä–∞–∑–¥–µ–ª–∞"

def create_topic_summary(paragraphs: List[str]) -> str:
    """–†–µ–∑—é–º–∏—Ä—É–µ–º –∞–±–∑–∞—Ü—ã"""
    key_sentences = []
    
    for para in paragraphs:
        sentences = sent_tokenize(para)
        for sent in sentences:
            if any(marker in sent.lower() for marker in [
                '—ç—Ç–æ', '—è–≤–ª—è–µ—Ç—Å—è', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç', '–æ–∑–Ω–∞—á–∞–µ—Ç',
                '–ø–æ–∑–≤–æ–ª—è–µ—Ç', '–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è', '–ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è',
                '–º–æ–∂–Ω–æ', '—Å–ª–µ–¥—É–µ—Ç', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ'
            ]):
                key_sentences.append(sent)
                if len(key_sentences) >= 2:
                    break
    
    if not key_sentences:
        for para in paragraphs:
            sentences = sent_tokenize(para)
            for sent in sentences:
                if len(sent) > 30:
                    key_sentences.append(sent)
                    break
            if key_sentences:
                break
    
    summary = " ".join(key_sentences[:2])
    summary = re.sub(r'\$[^$]+\$', '[—Ñ–æ—Ä–º—É–ª–∞]', summary)
    summary = re.sub(r'[^\w\s\[\].,!?;:()-]', '', summary)
    
    return summary[:300]

def extract_subtopics_smart(paragraphs: List[str]) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–¥-—Ç–æ–ø–∏–∫–∏"""
    subtopics = []
    
    for para in paragraphs:
        if re.search(r'(?:–≤–∫–ª—é—á–∞–µ—Ç|—Å–æ–¥–µ—Ä–∂–∏—Ç|—Å–æ—Å—Ç–æ–∏—Ç –∏–∑|—Ä–∞–∑–ª–∏—á–∞—é—Ç|–≤—ã–¥–µ–ª—è—é—Ç):', para, re.IGNORECASE):
            parts = para.split(':')
            if len(parts) > 1:
                items = re.split(r'[;,]', parts[1])
                for item in items[:4]:
                    item = item.strip()
                    if 10 < len(item) < 100:
                        subtopics.append(item)
        
        numbered = re.findall(r'\d+\)\s*([^.]+)\.', para)
        subtopics.extend([item.strip() for item in numbered[:4] if 10 < len(item) < 100])
    
    seen = set()
    unique_subtopics = []
    for topic in subtopics:
        if topic.lower() not in seen:
            seen.add(topic.lower())
            unique_subtopics.append(topic)
    
    return unique_subtopics[:5]

def extract_meaningful_examples(paragraphs: List[str]) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã"""
    examples = []
    
    for para in paragraphs:
        sentences = sent_tokenize(para)
        for sent in sentences:
            if any(indicator in sent.lower() for indicator in [
                '–Ω–∞–ø—Ä–∏–º–µ—Ä', '–∫ –ø—Ä–∏–º–µ—Ä—É', '–≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏', '—Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º',
                '–ø—É—Å—Ç—å', '–¥–æ–ø—É—Å—Ç–∏–º', '–ø—Ä–µ–¥—Å—Ç–∞–≤–∏–º', '–≤–æ–∑—å–º–µ–º'
            ]):
                example = sent.strip()
                for indicator in ['–ù–∞–ø—Ä–∏–º–µ—Ä,', '–ö –ø—Ä–∏–º–µ—Ä—É,', '–í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏,']:
                    example = example.replace(indicator, '').strip()
                
                if len(example) > 20:
                    examples.append(example)
            
            elif re.search(r'\d+\s*(?:%|–ø—Ä–æ—Ü–µ–Ω—Ç|–æ–±—ä–µ–∫—Ç|—ç–ª–µ–º–µ–Ω—Ç|–∫–ª–∞—Å—Å)', sent) and len(sent) < 200:
                examples.append(sent.strip())
    
    return examples[:5]

def extract_smart_relationships(topics: List[Dict], text: str) -> List[Dict]:
    """–ò–∑–≤–ª–µ–∫–∞–µ–º —Å–≤—è–∑—å –º–µ–∂–¥—É —Ç–æ–ø–∏–∫–∞–º–∏"""
    relationships = []
    
    topic_keywords = {}
    for i, topic in enumerate(topics):
        keywords = set()
        keywords.update(word_tokenize(topic['title'].lower()))
        for concept in topic['key_concepts']:
            keywords.update(word_tokenize(concept.lower()))
        topic_keywords[i] = keywords
    
    relationship_patterns = {
        'causes': ['–ø—Ä–∏–≤–æ–¥–∏—Ç –∫', '–≤—ã–∑—ã–≤–∞–µ—Ç', '–≤–ª–∏—è–µ—Ç –Ω–∞', '–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç'],
        'requires': ['—Ç—Ä–µ–±—É–µ—Ç', '–Ω–µ–æ–±—Ö–æ–¥–∏–º', '–Ω—É–∂–µ–Ω –¥–ª—è', '–æ—Å–Ω–æ–≤–∞–Ω –Ω–∞'],
        'part_of': ['—á–∞—Å—Ç—å', '–≤–∫–ª—é—á–∞–µ—Ç', '—Å–æ—Å—Ç–æ–∏—Ç –∏–∑', '—Å–æ–¥–µ—Ä–∂–∏—Ç'],
        'contrast': ['–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç', '–Ω–∞–ø—Ä–æ—Ç–∏–≤', '–æ–¥–Ω–∞–∫–æ', '–Ω–æ'],
        'similar': ['–ø–æ—Ö–æ–∂', '–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ', '—Ç–∞–∫–∂–µ –∫–∞–∫', '–ø–æ–¥–æ–±–Ω–æ']
    }
    
    sentences = sent_tokenize(text.lower())
    
    for i, topic1 in enumerate(topics):
        for j, topic2 in enumerate(topics):
            if i >= j:
                continue
            
            for sent in sentences:
                t1_found = any(kw in sent for kw in topic_keywords[i])
                t2_found = any(kw in sent for kw in topic_keywords[j])
                
                if t1_found and t2_found:
                    for rel_type, patterns in relationship_patterns.items():
                        if any(pattern in sent for pattern in patterns):
                            relationships.append({
                                "from": topic1['title'],
                                "to": topic2['title'],
                                "type": rel_type,
                                "description": f"{topic1['title']} {rel_type} {topic2['title']}"
                            })
                            break
                    
                    break
    
    return relationships[:10]

def extract_learning_objectives(topics: List[Dict]) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–º—ã –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –∏–∑ —Ç–æ–ø–∏–∫–æ–≤"""
    objectives = []
    
    objective_verbs = [
        "–ü–æ–Ω–∏–º–∞—Ç—å", "–û–±—ä—è—Å–Ω—è—Ç—å", "–ü—Ä–∏–º–µ–Ω—è—Ç—å", "–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å",
        "–†–∞–∑–ª–∏—á–∞—Ç—å", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å", "–í—ã—á–∏—Å–ª—è—Ç—å", "–û–ø—Ä–µ–¥–µ–ª—è—Ç—å"
    ]
    
    for topic in topics[:5]:
        verb = objective_verbs[len(objectives) % len(objective_verbs)]
        
        if topic['complexity'] == 'basic':
            objective = f"{verb} –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è {topic['title'].lower()}"
        elif topic['complexity'] == 'intermediate':
            objective = f"{verb} –∏ –ø—Ä–∏–º–µ–Ω—è—Ç—å {topic['title'].lower()}"
        else:
            objective = f"{verb} –∏ —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ —Ç–µ–º–µ {topic['title'].lower()}"
        
        objectives.append(objective)
    
    objectives.extend([
        "–†–µ—à–∞—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –ø–æ –∏–∑—É—á–µ–Ω–Ω—ã–º —Ç–µ–º–∞–º",
        "–°–≤—è–∑—ã–≤–∞—Ç—å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º"
    ])
    
    return objectives[:7]

def extract_key_concepts(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏"""
    concepts = []
    
    defined_terms = re.findall(
        r'(?:([–ê-–Ø][–∞-—è]+(?:\s+[–∞-—è]+){0,2})\s*(?:‚Äî|—ç—Ç–æ|–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è|—è–≤–ª—è–µ—Ç—Å—è))',
        text
    )
    concepts.extend([term.strip() for term in defined_terms if len(term) > 5])
    
    parenthetical = re.findall(r'\(([A-Za-z]+(?:\s+[A-Za-z]+){0,2})\)', text)
    concepts.extend([term for term in parenthetical if len(term) > 3])
    
    emphasized = re.findall(r'¬´([^¬ª]+)¬ª', text)
    concepts.extend([term for term in emphasized if 5 < len(term) < 50])
    
    words = word_tokenize(text.lower())
    meaningful_words = [
        w for w in words 
        if len(w) > 4 and w.isalpha() and
        not w in ['–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–±—É–¥–µ—Ç', '–º–æ–∂–µ—Ç', '—ç—Ç–æ–≥–æ', '—ç—Ç–æ–º—É']
    ]
    
    word_freq = Counter(meaningful_words)
    
    for word, freq in word_freq.most_common(20):
        if freq > 3 and word not in [c.lower() for c in concepts]:
            if any(pattern in text.lower() for pattern in [
                f'{word} —ç—Ç–æ',
                f'{word} —è–≤–ª—è–µ—Ç—Å—è',
                f'{word} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç',
                f'–∏—Å–ø–æ–ª—å–∑—É—è {word}',
                f'–º–µ—Ç–æ–¥–æ–º {word}'
            ]):
                concepts.append(word.capitalize())
    
    seen = set()
    unique_concepts = []
    for concept in concepts:
        if concept.lower() not in seen:
            seen.add(concept.lower())
            unique_concepts.append(concept)
    
    return unique_concepts[:15]

def determine_complexity(text: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
    words = word_tokenize(text.lower())
    
    # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    basic_words = ['–æ—Å–Ω–æ–≤–Ω–æ–π', '–ø—Ä–æ—Å—Ç–æ–π', '–±–∞–∑–æ–≤—ã–π', '—ç–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω—ã–π', '–Ω–∞—á–∞–ª—å–Ω—ã–π']
    intermediate_words = ['–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ', '–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ', '–∞–ª–≥–æ—Ä–∏—Ç–º', '–º–µ—Ç–æ–¥', '–∞–Ω–∞–ª–∏–∑']
    advanced_words = ['–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è', '–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ', '—Ç–µ–æ—Ä–µ–º–∞', '—Å–ª–æ–∂–Ω–æ—Å—Ç—å', '–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è']
    
    basic_count = sum(1 for w in words if w in basic_words)
    intermediate_count = sum(1 for w in words if w in intermediate_words)
    advanced_count = sum(1 for w in words if w in advanced_words)
    
    # –ò—â–µ–º —Ñ–æ—Ä–º—É–ª—ã
    formula_count = len(re.findall(r'[‚àë‚à´‚àÇ‚àá‚àà‚àÄ‚àÉ]|\$[^$]+\$', text))
    
    sentences = sent_tokenize(text)
    avg_sentence_length = np.mean([len(word_tokenize(s)) for s in sentences]) if sentences else 0
    
    if advanced_count > 2 or formula_count > 5 or avg_sentence_length > 25:
        return "advanced"
    elif intermediate_count > 2 or formula_count > 2 or avg_sentence_length > 20:
        return "intermediate"
    else:
        return "basic"

def generate_summary(text: str) -> str:
    """–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å GPT"""
    try:
        if not openai_client:
            load_models()
        
        max_chars = 128000
        if len(text) > max_chars:
            text = text[:max_chars] + "..."
        
        prompt = """–°–æ–∑–¥–∞–π –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ò–ù–§–û–†–ú–ê–¢–ò–í–ù–û–ï –∏ –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–û–ï —Ä–µ–∑—é–º–µ —É—á–µ–±–Ω–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:

üéØ –ì–õ–ê–í–ù–ê–Ø –ò–î–ï–Ø:
[2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –æ—Å–Ω–æ–≤–Ω–æ–π –º—ã—Å–ª—å—é –≤—Å–µ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞. –û–±—ä—è—Å–Ω–∏ –°–£–¢–¨ —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, –∞ –Ω–µ –ø–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞–π]

üìä –ö–õ–Æ–ß–ï–í–´–ï –ö–û–ù–¶–ï–ü–¶–ò–ò:
‚Ä¢ –ö–æ–Ω—Ü–µ–ø—Ü–∏—è 1: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ + –∑–∞—á–µ–º –Ω—É–∂–Ω–∞ + –≥–¥–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è
‚Ä¢ –ö–æ–Ω—Ü–µ–ø—Ü–∏—è 2: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ + –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç + –ø—Ä–∏–º–µ—Ä—ã
‚Ä¢ –ö–æ–Ω—Ü–µ–ø—Ü–∏—è 3: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ + —Å–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏
[4-6 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π]

üîó –í–ó–ê–ò–ú–û–°–í–Ø–ó–ò –ò –õ–û–ì–ò–ö–ê:
‚Ä¢ –ö–∞–∫ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è A –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ B (–ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–∞—è —Å–≤—è–∑—å)
‚Ä¢ –ü–æ—á–µ–º—É –º–µ—Ç–æ–¥ X –ª—É—á—à–µ –º–µ—Ç–æ–¥–∞ Y –≤ —Å–∏—Ç—É–∞—Ü–∏–∏ Z
‚Ä¢ –ö–∞–∫–∏–µ —É—Å–ª–æ–≤–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ C
[3-4 –≤–∞–∂–Ω–µ–π—à–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏]

üí° –ü–†–ê–ö–¢–ò–ß–ï–°–ö–û–ï –ü–†–ò–ú–ï–ù–ï–ù–ò–ï:
‚Ä¢ –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è 1: –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –∫–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ —Ä–µ—à–∞–µ—Ç
‚Ä¢ –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è 2: —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
‚Ä¢ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –∫–æ–≥–¥–∞ –ù–ï –ø—Ä–∏–º–µ–Ω—è—Ç—å
[2-3 –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∞—Å–ø–µ–∫—Ç–∞]

‚ö° –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –§–ê–ö–¢–´:
‚Ä¢ –ö–ª—é—á–µ–≤–æ–π —Ñ–∞–∫—Ç 1 (—á—Ç–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω—É–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å)
‚Ä¢ –ö–ª—é—á–µ–≤–æ–π —Ñ–∞–∫—Ç 2 (—á–∞—Å—Ç–∞—è –æ—à–∏–±–∫–∞ –∏–ª–∏ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ)
‚Ä¢ –ö–ª—é—á–µ–≤–æ–π —Ñ–∞–∫—Ç 3 (—É—Å–ª–æ–≤–∏–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è)
‚Ä¢ –ö–ª—é—á–µ–≤–æ–π —Ñ–∞–∫—Ç 4 (—Å–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ–º–∞–º–∏)

üßÆ –§–û–†–ú–£–õ–´ –ò –ê–õ–ì–û–†–ò–¢–ú–´ (–µ—Å–ª–∏ –µ—Å—Ç—å):
‚Ä¢ –û—Å–Ω–æ–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞: —á—Ç–æ –≤—ã—á–∏—Å–ª—è–µ—Ç, –∫–æ–≥–¥–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å
‚Ä¢ –ê–ª–≥–æ—Ä–∏—Ç–º: –ø–æ—à–∞–≥–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ, –≤—Ö–æ–¥–Ω—ã–µ/–≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
‚Ä¢ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: —á—Ç–æ –æ–∑–Ω–∞—á–∞—é—Ç, –∫–∞–∫ –≤—ã–±–∏—Ä–∞—Ç—å

üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò –í–´–í–û–î–´:
‚Ä¢ –ß—Ç–æ –º—ã –ø–æ–ª—É—á–∞–µ–º –≤ –∏—Ç–æ–≥–µ –∏–∑—É—á–µ–Ω–∏—è —ç—Ç–æ–π —Ç–µ–º—ã
‚Ä¢ –ö–∞–∫–∏–µ –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ç–∫—Ä—ã–≤–∞—é—Ç—Å—è
‚Ä¢ –ö –∫–∞–∫–∏–º —Å–ª–µ–¥—É—é—â–∏–º —Ç–µ–º–∞–º —ç—Ç–æ –≤–µ–¥–µ—Ç

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –ú–ê–ö–°–ò–ú–£–ú –ò–ù–§–û–†–ú–ê–¶–ò–ò –≤ –∫–∞–∂–¥–æ–º –ø—É–Ω–∫—Ç–µ
- –ù–ï –∫–æ–ø–∏—Ä—É–π —Ç–µ–∫—Å—Ç, –∞ –û–ë–™–Ø–°–ù–Ø–ô —Å—É—Ç—å
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏ —á–∏—Å–ª–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
- –£–∫–∞–∑—ã–≤–∞–π –ø—Ä–∏—á–∏–Ω—ã –∏ —Å–ª–µ–¥—Å—Ç–≤–∏—è
- –ú–∞–∫—Å–∏–º—É–º 400 —Å–ª–æ–≤ –æ–±—â–∏–π –æ–±—ä–µ–º
- –ö–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª –¥–æ–ª–∂–µ–Ω –¥–∞–≤–∞—Ç—å –Ω–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ
- –§–æ–∫—É—Å –Ω–∞ –ü–û–ù–ò–ú–ê–ù–ò–ò —Å–≤—è–∑–µ–π –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏
- –ü–∏—à–∏ –ø—Ä–æ—Å—Ç—ã–º, –Ω–æ —Ç–æ—á–Ω—ã–º —è–∑—ã–∫–æ–º"""
        
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. –ù–ï –∫–æ–ø–∏—Ä—É–π —Ç–µ–∫—Å—Ç, –∞ –æ–±—ä—è—Å–Ω—è–π —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏."},
                {"role": "user", "content": f"{prompt}\n\n–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{text}"}
            ],
            temperature=0.7,
            max_tokens=800
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"Error generating advanced summary: {str(e)}")
        return "## üéØ –ì–ª–∞–≤–Ω–∞—è –∏–¥–µ—è\n–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –∏–∑-–∑–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–∏."

def generate_flashcards(text: str) -> List[Dict]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–ª–µ—à-–∫–∞—Ä—Ç—ã —Å GPT"""
    try:
        if not openai_client:
            load_models()
        
        max_chars = 128000
        if len(text) > max_chars:
            text = text[:max_chars] + "..."
        
        prompt = """–°–æ–∑–¥–∞–π –ú–ù–û–ì–û–£–†–û–í–ù–ï–í–´–ï —Ñ–ª–µ—à-–∫–∞—Ä—Ç—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è –î–ê–ù–ù–û–ì–û –ö–û–ù–ö–†–ï–¢–ù–û–ì–û –¢–ï–ö–°–¢–ê.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
1. –í–°–ï –æ—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –¢–û–õ–¨–ö–û –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç - –ø–∏—à–∏ "–í —Ç–µ–∫—Å—Ç–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ" –∏–ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏ –∫–∞—Ä—Ç–æ—á–∫—É
3. –ù–ï –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å–≤–æ–∏—Ö –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π
4. –¶–∏—Ç–∏—Ä—É–π –∏–ª–∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π –¢–û–õ–¨–ö–û —Ç–æ, —á—Ç–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
5. –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å —Ñ–æ—Ä–º—É–ª—ã, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –ø—Ä–∏–º–µ—Ä—ã - –∏—Å–ø–æ–ª—å–∑—É–π –∏–º–µ–Ω–Ω–æ –∏—Ö

–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 15 –∫–∞—Ä—Ç–æ—á–µ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ö–ê–ñ–î–û–ô –∫–∞—Ä—Ç–æ—á–∫–∏:
{
  "type": "—Ç–∏–ø –∫–∞—Ä—Ç–æ—á–∫–∏",
  "q": "–û–°–ú–´–°–õ–ï–ù–ù–´–ô –≤–æ–ø—Ä–æ—Å –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞",
  "a": "–û—Ç–≤–µ—Ç, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –°–¢–†–û–ì–û –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, –Ω–æ —Ç–æ—á–Ω—ã–π)",
  "hint": "–ø–æ–¥—Å–∫–∞–∑–∫–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ —É–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ä–∞–∑–¥–µ–ª",
  "difficulty": —á–∏—Å–ª–æ –æ—Ç 1 –¥–æ 3,
  "related_topics": ["—Ç–µ–º–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ 1", "—Ç–µ–º–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ 2"],
  "memory_hook": "–∞—Å—Å–æ—Ü–∏–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
  "common_mistakes": "—á—Ç–æ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–ø—É—Ç–∞—Ç—å —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ–∫—Å—Ç—É",
  "text_reference": "–∫—Ä–∞—Ç–∫–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –º–µ—Å—Ç–æ –≤ —Ç–µ–∫—Å—Ç–µ, –æ—Ç–∫—É–¥–∞ –≤–∑—è—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"
}

–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º (—Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏):
1. "definition" (3 –∫–∞—Ä—Ç—ã) - –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ò–ó –¢–ï–ö–°–¢–ê, difficulty: 1
   –ü—Ä–∏–º–µ—Ä: "–ö–∞–∫ –≤ —Ç–µ–∫—Å—Ç–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ (accuracy)?"
   –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ñ–æ—Ä–º—É–ª—É/–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞
   
2. "concept" (4 –∫–∞—Ä—Ç—ã) - –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –ò–ó –¢–ï–ö–°–¢–ê, difficulty: 2
   –ü—Ä–∏–º–µ—Ä: "–ü–æ—á–µ–º—É —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ–∫—Å—Ç—É —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª (1.1) –Ω–µ–ª—å–∑—è –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏?"
   –û—Ç–≤–µ—Ç: —Ç–æ, —á—Ç–æ –Ω–∞–ø–∏—Å–∞–Ω–æ –≤ —Ç–µ–∫—Å—Ç–µ –æ–± —ç—Ç–æ–º
   
3. "application" (3 –∫–∞—Ä—Ç—ã) - –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –û–ü–ò–°–ê–ù–û –í –¢–ï–ö–°–¢–ï, difficulty: 2
   –ü—Ä–∏–º–µ—Ä: "–ö–∞–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ?"
   –û—Ç–≤–µ—Ç: —Ç–æ–ª—å–∫–æ —Ç–µ –ø—Ä–∏–º–µ—Ä—ã, —á—Ç–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
   
4. "comparison" (3 –∫–∞—Ä—Ç—ã) - —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –£–ü–û–ú–Ø–ù–£–¢–´–ï –í –¢–ï–ö–°–¢–ï, difficulty: 3
   –ü—Ä–∏–º–µ—Ä: "–ö–∞–∫ —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ–∫—Å—Ç—É —Å–≤—è–∑–∞–Ω—ã —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–Ω–æ—Ç–∞?"
   –û—Ç–≤–µ—Ç: —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —Å–∫–∞–∑–∞–Ω–æ –≤ —Ç–µ–∫—Å—Ç–µ
   
5. "problem" (2 –∫–∞—Ä—Ç—ã) - –∑–∞–¥–∞—á–∏/–ø—Ä–∏–º–µ—Ä—ã –ò–ó –¢–ï–ö–°–¢–ê, difficulty: 3
   –ü—Ä–∏–º–µ—Ä: "–†–µ—à–∏—Ç–µ –ø—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞: [–µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä]"

–ü–†–û–í–ï–†–ö–ê: –ü–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ —É–±–µ–¥–∏—Å—å, —á—Ç–æ:
- –í–æ–ø—Ä–æ—Å –∫–∞—Å–∞–µ—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ï–°–¢–¨ –≤ —Ç–µ–∫—Å—Ç–µ
- –û—Ç–≤–µ—Ç –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –∏–ª–∏ –≤—ã–≤–µ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
- –¢—ã –Ω–µ –¥–æ–±–∞–≤–ª—è–µ—à—å —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è –æ —Ç–µ–º–µ

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –º–∞—Å—Å–∏–≤."""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã —Å–æ–∑–¥–∞–µ—à—å —Ñ–ª–µ—à-–∫–∞—Ä—Ç—ã –°–¢–†–û–ì–û –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π —Å–≤–æ–∏ –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –æ —Ç–µ–º–µ. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ - –Ω–µ —Å–æ–∑–¥–∞–≤–∞–π –∫–∞—Ä—Ç–æ—á–∫—É. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–º JSON."},
                {"role": "user", "content": f"{prompt}\n\n–¢–µ–∫—Å—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—Ä—Ç–æ—á–µ–∫:\n{text}"}
            ],
            temperature=0.3,  # –°–Ω–∏–∑–∏–ª —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç—É
            max_tokens=3000
        )
        
        content = response.choices[0].message.content.strip()
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON
        json_match = re.search(r'\[.*\]', content, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
        else:
            json_str = content
        
        flashcards = json.loads(json_str)
        
        validated_cards = []
        for card in flashcards:
            if 'q' in card and 'a' in card:
                if 'text_reference' not in card:
                    card['text_reference'] = "–°–º. —Ç–µ–∫—Å—Ç –≤—ã—à–µ"
                
                card['next_review'] = calculate_next_review(card.get('difficulty', 2))
                card['ease_factor'] = 2.5
                
                validated_cards.append(card)
        
        return validated_cards if validated_cards else generate_fallback_flashcards(text)
        
    except Exception as e:
        logger.error(f"Error generating advanced flashcards: {str(e)}")
        return generate_fallback_flashcards(text)

def generate_fallback_flashcards(text: str) -> List[Dict]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–ª–µ—à-–∫–∞—Ä—Ç –±–µ–∑ GPT"""
    flashcards = []
    
    try:
        definition_patterns = [
            r'([–ê-–Ø][–∞-—è]+(?:\s+[–∞-—è]+){0,3})\s+(?:‚Äî|‚Äì|-|—ç—Ç–æ|—è–≤–ª—è–µ—Ç—Å—è)\s*([^.]+)\.',
            r'([–ê-–Ø][–∞-—è]+(?:\s+[–∞-—è]+){0,3})\s+–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è\s+([^.]+)\.',
            r'–ü–æ–¥\s+([–∞-—è]+(?:\s+[–∞-—è]+){0,3})\s+–ø–æ–Ω–∏–º–∞—é—Ç\s+([^.]+)\.'
        ]
        
        for pattern in definition_patterns:
            matches = re.findall(pattern, text, re.MULTILINE)
            for term, definition in matches[:3]:
                if len(term) > 3 and len(definition) > 10:
                    flashcards.append({
                        "type": "definition",
                        "q": f"–ß—Ç–æ —Ç–∞–∫–æ–µ {term.strip()}?",
                        "a": f"{term} ‚Äî —ç—Ç–æ {definition.strip()}",
                        "hint": f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–æ –≤ —Ç–µ–∫—Å—Ç–µ",
                        "difficulty": 1,
                        "related_topics": [term.strip()],
                        "memory_hook": f"–ó–∞–ø–æ–º–Ω–∏: {term} = {definition.split()[0]}...",
                        "common_mistakes": "–ù–µ –ø—É—Ç–∞—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏",
                        "text_reference": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
                        "next_review": calculate_next_review(1),
                        "ease_factor": 2.5
                    })
        
        formula_matches = re.findall(r'([A-Za-z]+\([^)]+\))\s*=\s*([^.]+)', text)
        for formula_name, formula_body in formula_matches[:2]:
            flashcards.append({
                "type": "concept",
                "q": f"–ö–∞–∫ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è {formula_name}?",
                "a": f"{formula_name} = {formula_body.strip()}",
                "hint": "–§–æ—Ä–º—É–ª–∞ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ",
                "difficulty": 2,
                "related_topics": ["—Ñ–æ—Ä–º—É–ª—ã", "–≤—ã—á–∏—Å–ª–µ–Ω–∏—è"],
                "memory_hook": f"–§–æ—Ä–º—É–ª–∞: {formula_name}",
                "common_mistakes": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Å–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º—É–ª–µ",
                "text_reference": "–§–æ—Ä–º—É–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
                "next_review": calculate_next_review(2),
                "ease_factor": 2.5
            })
        
        example_sentences = re.findall(r'(?:–ù–∞–ø—Ä–∏–º–µ—Ä|–ö –ø—Ä–∏–º–µ—Ä—É|–ü—Ä–∏–º–µ—Ä)[,:]?\s*([^.]+)\.', text, re.IGNORECASE)
        for i, example in enumerate(example_sentences[:2]):
            flashcards.append({
                "type": "application",
                "q": f"–ü—Ä–∏–≤–µ–¥–∏—Ç–µ –ø—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞ #{i+1}",
                "a": example.strip(),
                "hint": "–ü—Ä–∏–º–µ—Ä —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –≤ —Ç–µ–∫—Å—Ç–µ",
                "difficulty": 2,
                "related_topics": ["–ø—Ä–∏–º–µ—Ä—ã", "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ"],
                "memory_hook": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞",
                "common_mistakes": "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø—Ä–∏–º–µ—Ä –ø–æ–ª–Ω—ã–π",
                "text_reference": "–ü—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞",
                "next_review": calculate_next_review(2),
                "ease_factor": 2.5
            })
        
        key_statements = re.findall(r'(?:–í–∞–∂–Ω–æ|–°–ª–µ–¥—É–µ—Ç|–ù–µ–æ–±—Ö–æ–¥–∏–º–æ|–ù—É–∂–Ω–æ)\s+([^.]+)\.', text, re.IGNORECASE)
        for statement in key_statements[:2]:
            flashcards.append({
                "type": "concept",
                "q": f"–ß—Ç–æ –≤–∞–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ–∫—Å—Ç—É?",
                "a": statement.strip(),
                "hint": "–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è",
                "difficulty": 2,
                "related_topics": ["–∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã"],
                "memory_hook": "–í–∞–∂–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ",
                "common_mistakes": "–ù–µ —É–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç",
                "text_reference": "–í–∞–∂–Ω–æ–µ –∑–∞–º–µ—á–∞–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
                "next_review": calculate_next_review(2),
                "ease_factor": 2.5
            })
        
        if len(flashcards) < 5:
            sentences = sent_tokenize(text)
            informative_sentences = [s for s in sentences if len(s) > 50 and not s.endswith('?')]
            
            for i, sent in enumerate(informative_sentences[:5-len(flashcards)]):
                flashcards.append({
                    "type": "concept",
                    "q": f"–û–±—ä—è—Å–Ω–∏—Ç–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞: {sent[:50]}...?",
                    "a": sent.strip(),
                    "hint": "–û—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ —Å–∞–º–æ–º –≤–æ–ø—Ä–æ—Å–µ",
                    "difficulty": 1,
                    "related_topics": ["–æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è"],
                    "memory_hook": "–ö–ª—é—á–µ–≤–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ",
                    "common_mistakes": "–ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ",
                    "text_reference": "–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
                    "next_review": calculate_next_review(1),
                    "ease_factor": 2.5
                })
        
        return flashcards
        
    except Exception as e:
        logger.error(f"Error in fallback flashcard generation: {str(e)}")
        return [{
            "type": "definition",
            "q": "–û —á–µ–º —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç?",
            "a": "–¢–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —É—á–µ–±–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è. –ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π.",
            "hint": "–ü–æ–¥—É–º–∞–π—Ç–µ –æ –≥–ª–∞–≤–Ω–æ–π —Ç–µ–º–µ",
            "difficulty": 1,
            "related_topics": ["–æ—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞"],
            "memory_hook": "–ì–ª–∞–≤–Ω–∞—è –∏–¥–µ—è —Ç–µ–∫—Å—Ç–∞",
            "common_mistakes": "–ù–µ –ø—É—Ç–∞—Ç—å –¥–µ—Ç–∞–ª–∏ —Å –≥–ª–∞–≤–Ω–æ–π –∏–¥–µ–µ–π",
            "text_reference": "–í–µ—Å—å —Ç–µ–∫—Å—Ç",
            "next_review": calculate_next_review(1),
            "ease_factor": 2.5
        }]

def calculate_next_review(difficulty: int) -> str:
    """–°—á–∏—Ç–∞–µ–º –¥–µ–Ω—å —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    days_map = {1: 1, 2: 3, 3: 7}
    days = days_map.get(difficulty, 3)
    next_date = datetime.now() + timedelta(days=days)
    return next_date.strftime("%Y-%m-%d")

def generate_mind_map(text: str, topics: List[Dict]) -> Dict:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Mind Map"""
    try:
        if topics and 'title' in topics[0]:
            central_topic = topics[0]['title']
        else:
            central_topic = "–û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞"
        
        branches = []
        colors = ["#FF6B6B", "#4ECDC4", "#45B7D1", "#96CEB4", "#FECA57", "#48DBFB"]
        
        for i, topic in enumerate(topics[:6]):
            branch = {
                "name": topic['title'][:40],
                "importance": 0.9 - (i * 0.1),
                "color": colors[i % len(colors)],
                "children": []
            }
            
            for subtopic in topic.get('subtopics', [])[:3]:
                branch['children'].append({
                    "name": subtopic[:30],
                    "importance": 0.5,
                    "color": branch['color']
                })
            
            if not branch['children'] and topic.get('key_concepts'):
                for concept in topic['key_concepts'][:3]:
                    branch['children'].append({
                        "name": concept[:30],
                        "importance": 0.4,
                        "color": branch['color']
                    })
            
            branches.append(branch)
        
        return {
            "central_topic": central_topic,
            "branches": branches
        }
        
    except Exception as e:
        logger.error(f"Error generating mind map: {str(e)}")
        return {
            "central_topic": "–¢–µ–º–∞",
            "branches": []
        }

def generate_study_plan(topics: List[Dict], flashcards: List[Dict], text_length: int = 0) -> Dict:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω –æ–±—É—á–µ–Ω–∏—è —Å –Ω–∞—É—á–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º"""
    try:
        # –ê–Ω–∞–ª–∏–∑ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
        material_analysis = _analyze_material_complexity(topics, flashcards, text_length)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
        schedule_config = _calculate_optimal_schedule(material_analysis)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑—É—á–µ–Ω–∏—è
        learning_sequence = _create_learning_sequence(topics, material_analysis)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ—Å—Å–∏–π
        sessions = _generate_study_sessions(learning_sequence, flashcards, schedule_config)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
        review_system = _create_spaced_repetition_schedule(sessions, material_analysis)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫
        milestones = _generate_smart_milestones(topics, sessions, material_analysis)
        
        return {
            "sessions": sessions,
            "milestones": milestones,
            "review_schedule": review_system["intervals"],
            "total_hours": schedule_config["total_hours"],
            "completion_date": schedule_config["completion_date"],
            "difficulty_level": material_analysis["overall_difficulty"],
            "material_analysis": material_analysis,
            "adaptive_features": {
                "difficulty_adjustment": True,
                "personalized_pace": True,
                "progress_tracking": True
            },
            "success_metrics": {
                "target_retention": 85,
                "target_completion": 90,
                "target_satisfaction": 8
            }
        }
        
    except Exception as e:
        logger.error(f"Error generating enhanced study plan: {str(e)}")
        return _generate_fallback_study_plan()

def _analyze_material_complexity(topics: List[Dict], flashcards: List[Dict], text_length: int) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞"""
    
    # –ê–Ω–∞–ª–∏–∑ —Ç–µ–º –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    complexity_distribution = {"basic": 0, "intermediate": 0, "advanced": 0}
    topic_depths = []
    
    for topic in topics:
        complexity = topic.get('complexity', 'basic')
        complexity_distribution[complexity] += 1
        
        # –û—Ü–µ–Ω–∫–∞ –≥–ª—É–±–∏–Ω—ã —Ç–µ–º—ã
        depth_score = 0
        depth_score += len(topic.get('subtopics', [])) * 0.3
        depth_score += len(topic.get('key_concepts', [])) * 0.2
        depth_score += len(topic.get('examples', [])) * 0.1
        depth_score += len(topic.get('summary', '')) / 100
        
        topic_depths.append(depth_score)
    
    # –ê–Ω–∞–ª–∏–∑ —Ñ–ª–µ—à-–∫–∞—Ä—Ç
    card_difficulties = [card.get('difficulty', 1) for card in flashcards]
    avg_card_difficulty = sum(card_difficulties) / len(card_difficulties) if card_difficulties else 1
    
    # –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
    volume_factor = min(2.0, text_length / 10000)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –æ–±—ä–µ–º—É
    
    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    complexity_weights = {"basic": 1, "intermediate": 2, "advanced": 3}
    weighted_complexity = sum(complexity_distribution[k] * v for k, v in complexity_weights.items())
    total_topics = sum(complexity_distribution.values())
    overall_difficulty = weighted_complexity / max(total_topics, 1)
    
    # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑—É—á–µ–Ω–∏—è
    base_time_per_topic = {1: 15, 2: 25, 3: 40}  # –º–∏–Ω—É—Ç—ã
    time_per_card = 2  # –º–∏–Ω—É—Ç—ã –Ω–∞ —Ñ–ª–µ—à-–∫–∞—Ä—Ç—É
    reading_time = (text_length / 5) / 200  # 200 —Å–ª–æ–≤ –≤ –º–∏–Ω—É—Ç—É
    
    topic_time = len(topics) * base_time_per_topic.get(int(overall_difficulty), 25)
    card_time = len(flashcards) * time_per_card
    total_minutes = (reading_time + topic_time + card_time) * 1.3  # +30% –Ω–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
    
    return {
        "complexity_distribution": complexity_distribution,
        "topic_depths": topic_depths,
        "avg_topic_depth": sum(topic_depths) / len(topic_depths) if topic_depths else 1,
        "avg_card_difficulty": avg_card_difficulty,
        "volume_factor": volume_factor,
        "overall_difficulty": overall_difficulty,
        "estimated_study_time": {
            "total_minutes": int(total_minutes),
            "total_hours": round(total_minutes / 60, 1),
            "reading_time": int(reading_time),
            "study_time": int(topic_time + card_time),
            "review_time": int(total_minutes * 0.3)
        }
    }

def _calculate_optimal_schedule(analysis: Dict) -> Dict:
    """–†–∞—Å—á–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è"""
    
    total_hours = analysis["estimated_study_time"]["total_hours"]
    difficulty = analysis["overall_difficulty"]
    
    # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    if difficulty < 1.5:
        session_duration = 30  # –ª–µ–≥–∫–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª
        sessions_per_week = 4
    elif difficulty < 2.5:
        session_duration = 45  # —Å—Ä–µ–¥–Ω–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª
        sessions_per_week = 5
    else:
        session_duration = 60  # —Å–ª–æ–∂–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª
        sessions_per_week = 6
    
    # –†–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–µ—Å—Å–∏–π
    total_sessions = max(3, int(total_hours * 60 / session_duration))
    
    # –†–∞—Å—á–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫—É—Ä—Å–∞
    weeks_needed = max(1, total_sessions / sessions_per_week)
    completion_date = datetime.now() + timedelta(weeks=weeks_needed)
    
    return {
        "session_duration": session_duration,
        "total_sessions": total_sessions,
        "sessions_per_week": sessions_per_week,
        "weeks_needed": int(weeks_needed),
        "total_hours": total_hours,
        "completion_date": completion_date.strftime("%d.%m.%Y")
    }

def _create_learning_sequence(topics: List[Dict], analysis: Dict) -> Dict:
    """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑—É—á–µ–Ω–∏—è"""
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ç–µ–º –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º
    sorted_topics = sorted(topics, key=lambda t: (
        {"basic": 1, "intermediate": 2, "advanced": 3}.get(t.get('complexity', 'basic'), 2),
        -len(t.get('key_concepts', [])),  # –±–æ–ª—å—à–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π = –∏–∑—É—á–∞–µ–º —Ä–∞–Ω—å—à–µ
        len(t.get('title', ''))  # –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –æ–±—ã—á–Ω–æ –±–∞–∑–æ–≤—ã–µ
    ))
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ç–µ–º –ø–æ —Ñ–∞–∑–∞–º –æ–±—É—á–µ–Ω–∏—è
    total_topics = len(sorted_topics)
    foundation_phase = sorted_topics[:total_topics//3] if total_topics > 3 else sorted_topics[:1]
    development_phase = sorted_topics[total_topics//3:2*total_topics//3] if total_topics > 3 else sorted_topics[1:2]
    mastery_phase = sorted_topics[2*total_topics//3:] if total_topics > 3 else sorted_topics[2:]
    
    return {
        "foundation": foundation_phase,
        "development": development_phase,
        "mastery": mastery_phase,
        "all_topics": sorted_topics
    }

def _generate_study_sessions(sequence: Dict, flashcards: List[Dict], config: Dict) -> List[Dict]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —É—á–µ–±–Ω—ã—Ö —Å–µ—Å—Å–∏–π"""
    
    sessions = []
    all_topics = sequence["all_topics"]
    total_sessions = config["total_sessions"]
    session_duration = config["session_duration"]
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–ª–µ—à-–∫–∞—Ä—Ç –ø–æ —Å–µ—Å—Å–∏—è–º
    cards_per_session = max(3, len(flashcards) // total_sessions) if flashcards else 0
    
    for session_num in range(1, total_sessions + 1):
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è
        if session_num <= total_sessions // 3:
            phase = "foundation"
            phase_name = "–û—Å–Ω–æ–≤—ã"
            topics_pool = sequence["foundation"]
        elif session_num <= 2 * total_sessions // 3:
            phase = "development"
            phase_name = "–†–∞–∑–≤–∏—Ç–∏–µ"
            topics_pool = sequence["development"]
        else:
            phase = "mastery"
            phase_name = "–ú–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ"
            topics_pool = sequence["mastery"]
        
        # –í—ã–±–æ—Ä —Ç–µ–º –¥–ª—è —Å–µ—Å—Å–∏–∏
        topic_index = (session_num - 1) % len(topics_pool) if topics_pool else 0
        current_topic = topics_pool[topic_index] if topics_pool else {"title": "–û–±—â–µ–µ –∏–∑—É—á–µ–Ω–∏–µ", "complexity": "basic"}
        
        # –í—ã–±–æ—Ä —Ñ–ª–µ—à-–∫–∞—Ä—Ç
        start_card = (session_num - 1) * cards_per_session
        end_card = min(start_card + cards_per_session, len(flashcards))
        
        # –†–∞—Å—á–µ—Ç –¥–∞—Ç—ã —Å–µ—Å—Å–∏–∏
        days_from_start = (session_num - 1) * (7 / config["sessions_per_week"])
        session_date = datetime.now() + timedelta(days=days_from_start)
        
        session = {
            "day": session_num,
            "session_number": session_num,
            "date": session_date.strftime("%d.%m.%Y"),
            "day_of_week": ["–ü–Ω", "–í—Ç", "–°—Ä", "–ß—Ç", "–ü—Ç", "–°–±", "–í—Å"][session_date.weekday()],
            "phase": phase,
            "phase_name": phase_name,
            "duration_minutes": session_duration,
            "main_topic": current_topic,
            "topics": [current_topic["title"]],
            "flashcards_count": end_card - start_card,
            "flashcard_ids": list(range(start_card, end_card)),
            "focus": _get_session_focus(phase),
            "exercises": _generate_session_exercises(current_topic, phase),
            "learning_objectives": _generate_session_objectives(current_topic, phase),
            "success_criteria": _generate_success_criteria(current_topic, end_card - start_card),
            "estimated_difficulty": current_topic.get("complexity", "basic"),
            "activities": _generate_session_activities(current_topic, phase, session_duration)
        }
        
        sessions.append(session)
    
    return sessions

def _get_session_focus(phase: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ–∫—É—Å–∞ —Å–µ—Å—Å–∏–∏"""
    focus_map = {
        "foundation": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∏ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏",
        "development": "–£–≥–ª—É–±–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π",
        "mastery": "–ó–∞–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ"
    }
    return focus_map.get(phase, "–ò–∑—É—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞")

def _generate_session_exercises(topic: Dict, phase: str) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π –¥–ª—è —Å–µ—Å—Å–∏–∏"""
    
    topic_title = topic.get("title", "–∏–∑—É—á–µ–Ω–Ω—É—é —Ç–µ–º—É")
    
    if phase == "foundation":
        return [
            f"–û–±—ä—è—Å–Ω–∏—Ç–µ {topic_title} –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏",
            f"–ü—Ä–∏–≤–µ–¥–∏—Ç–µ 2-3 –ø—Ä–∏–º–µ—Ä–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è '{topic_title}'",
            f"–°–æ–∑–¥–∞–π—Ç–µ –ø—Ä–æ—Å—Ç—É—é —Å—Ö–µ–º—É –¥–ª—è '{topic_title}'"
        ]
    elif phase == "development":
        return [
            f"–°—Ä–∞–≤–Ω–∏—Ç–µ {topic_title} —Å —Ä–∞–Ω–µ–µ –∏–∑—É—á–µ–Ω–Ω—ã–º–∏ —Ç–µ–º–∞–º–∏",
            f"–†–µ—à–∏—Ç–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –∑–∞–¥–∞—á—É –ø–æ —Ç–µ–º–µ '{topic_title}'",
            f"–°–æ–∑–¥–∞–π—Ç–µ mind map –¥–ª—è '{topic_title}'"
        ]
    else:  # mastery
        return [
            f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏—Ç–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å '{topic_title}'",
            f"–°–æ–∑–¥–∞–π—Ç–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä –¥–ª—è '{topic_title}'",
            f"–û–±—ä—è—Å–Ω–∏—Ç–µ '{topic_title}' –Ω–æ–≤–∏—á–∫—É"
        ]

def _generate_session_objectives(topic: Dict, phase: str) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ü–µ–ª–µ–π —Å–µ—Å—Å–∏–∏"""
    
    topic_title = topic.get("title", "–º–∞—Ç–µ—Ä–∏–∞–ª")
    
    if phase == "foundation":
        return [
            f"–ü–æ–Ω—è—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ —Ç–µ–º—ã '{topic_title}'",
            "–ó–∞–ø–æ–º–Ω–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Ç–µ—Ä–º–∏–Ω—ã",
            "–£–º–µ—Ç—å –æ–±—ä—è—Å–Ω–∏—Ç—å —Ç–µ–º—É –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏"
        ]
    elif phase == "development":
        return [
            f"–£–≥–ª—É–±–∏—Ç—å –ø–æ–Ω–∏–º–∞–Ω–∏–µ '{topic_title}'",
            "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∏–∑—É—á–µ–Ω–Ω—ã–º–∏ —Ç–µ–º–∞–º–∏",
            "–ü—Ä–∏–º–µ–Ω–∏—Ç—å –∑–Ω–∞–Ω–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á"
        ]
    else:  # mastery
        return [
            f"–î–æ—Å—Ç–∏—á—å —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è '{topic_title}'",
            "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é",
            "–°–æ–∑–¥–∞–≤–∞—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è"
        ]

def _generate_success_criteria(topic: Dict, cards_count: int) -> List[str]:
    """–ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–µ—Å—Å–∏–∏"""
    
    return [
        f"–ü—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ {max(1, int(cards_count * 0.8))} –∏–∑ {cards_count} —Ñ–ª–µ—à-–∫–∞—Ä—Ç" if cards_count > 0 else "–ò–∑—É—á–∏—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –º–∞—Ç–µ—Ä–∏–∞–ª",
        "–û–±—ä—è—Å–Ω–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏",
        "–ü—Ä–∏–≤–µ—Å—Ç–∏ –º–∏–Ω–∏–º—É–º 1 –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è",
        "–û—Ü–µ–Ω–∏—Ç—å —Å–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–º—ã –Ω–∞ 7+ –±–∞–ª–ª–æ–≤ –∏–∑ 10"
    ]

def _generate_session_activities(topic: Dict, phase: str, duration: int) -> List[Dict]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π –¥–ª—è —Å–µ—Å—Å–∏–∏"""
    
    activities = []
    topic_title = topic.get("title", "–ò–∑—É—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞")
    
    # –†–∞–∑–º–∏–Ω–∫–∞ (5 –º–∏–Ω—É—Ç)
    activities.append({
        "type": "warmup",
        "name": "–ê–∫—Ç–∏–≤–∞—Ü–∏—è –∑–Ω–∞–Ω–∏–π",
        "duration": 5,
        "description": f"–í—Å–ø–æ–º–Ω–∏—Ç–µ, —á—Ç–æ –≤—ã —É–∂–µ –∑–Ω–∞–µ—Ç–µ –æ —Ç–µ–º–µ '{topic_title}'",
        "icon": "üß†"
    })
    
    # –û—Å–Ω–æ–≤–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ (50% –≤—Ä–µ–º–µ–Ω–∏)
    main_study_time = int(duration * 0.5)
    activities.append({
        "type": "study",
        "name": "–ò–∑—É—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞",
        "duration": main_study_time,
        "description": f"–ì–ª—É–±–æ–∫–æ–µ –∏–∑—É—á–µ–Ω–∏–µ —Ç–µ–º—ã '{topic_title}' —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏",
        "icon": "üìö"
    })
    
    # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ (25% –≤—Ä–µ–º–µ–Ω–∏)
    practice_time = int(duration * 0.25)
    activities.append({
        "type": "practice",
        "name": "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ",
        "duration": practice_time,
        "description": "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–∑—É—á–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ",
        "icon": "‚ö°"
    })
    
    # –†–µ—Ñ–ª–µ–∫—Å–∏—è (20% –≤—Ä–µ–º–µ–Ω–∏)
    reflection_time = int(duration * 0.2)
    activities.append({
        "type": "reflection",
        "name": "–†–µ—Ñ–ª–µ–∫—Å–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ",
        "duration": reflection_time,
        "description": "–û—Ü–µ–Ω–∫–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–æ–≤",
        "icon": "ü§î"
    })
    
    return activities

def _create_spaced_repetition_schedule(sessions: List[Dict], analysis: Dict) -> Dict:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∏–Ω—Ç–µ—Ä–≤–∞–ª—å–Ω—ã—Ö –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π"""
    
    # –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –ø–æ –≠–±–±–∏–Ω–≥–∞—É–∑—É (–¥–Ω–∏)
    difficulty = analysis["overall_difficulty"]
    if difficulty > 2.5:
        intervals = [1, 2, 5, 10, 21, 45]  # —á–∞—â–µ –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
    elif difficulty < 1.5:
        intervals = [2, 5, 10, 21, 45, 90]  # —Ä–µ–∂–µ –¥–ª—è –ª–µ–≥–∫–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
    else:
        intervals = [1, 3, 7, 14, 30, 60]  # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
    
    return {
        "intervals": intervals,
        "strategy": "spaced_repetition",
        "total_reviews": len(sessions) * len(intervals)
    }

def _generate_smart_milestones(topics: List[Dict], sessions: List[Dict], analysis: Dict) -> List[Dict]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–º–Ω—ã—Ö –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫"""
    
    milestones = []
    total_sessions = len(sessions)
    
    # Milestone 1: –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ—Å–Ω–æ–≤ (25% –ø—Ä–æ–≥—Ä–µ—Å—Å–∞)
    foundation_session = max(1, total_sessions // 4)
    milestones.append({
        "session": foundation_session,
        "title": "–û—Å–≤–æ–µ–Ω–∏–µ –æ—Å–Ω–æ–≤",
        "description": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∏ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏",
        "progress_percent": 25
    })
    
    # Milestone 2: –†–∞–∑–≤–∏—Ç–∏–µ –Ω–∞–≤—ã–∫–æ–≤ (50% –ø—Ä–æ–≥—Ä–µ—Å—Å–∞)
    development_session = max(2, total_sessions // 2)
    milestones.append({
        "session": development_session,
        "title": "–†–∞–∑–≤–∏—Ç–∏–µ –Ω–∞–≤—ã–∫–æ–≤",
        "description": "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π",
        "progress_percent": 50
    })
    
    # Milestone 3: –ú–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ (75% –ø—Ä–æ–≥—Ä–µ—Å—Å–∞)
    mastery_session = max(3, 3 * total_sessions // 4)
    milestones.append({
        "session": mastery_session,
        "title": "–î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞",
        "description": "–≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ",
        "progress_percent": 75
    })
    
    # Final Milestone: –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∫—É—Ä—Å–∞ (100% –ø—Ä–æ–≥—Ä–µ—Å—Å–∞)
    milestones.append({
        "session": total_sessions,
        "title": "–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∏–∑—É—á–µ–Ω–∏—è",
        "description": "–ü–æ–ª–Ω–æ–µ –æ—Å–≤–æ–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞",
        "progress_percent": 100
    })
    
    return milestones

def _generate_fallback_study_plan() -> Dict:
    """–ë–∞–∑–æ–≤—ã–π –ø–ª–∞–Ω –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏"""
    
    return {
        "sessions": [{
            "day": 1,
            "session_number": 1,
            "date": datetime.now().strftime("%d.%m.%Y"),
            "duration_minutes": 45,
            "topics": ["–ò–∑—É—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞"],
            "focus": "–ò–∑—É—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞",
            "exercises": ["–ü—Ä–æ—á–∏—Ç–∞—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª", "–°–¥–µ–ª–∞—Ç—å –∑–∞–º–µ—Ç–∫–∏", "–ü–æ–≤—Ç–æ—Ä–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã"],
            "main_topic": {"title": "–ò–∑—É—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞", "complexity": "basic"},
            "phase_name": "–û—Å–Ω–æ–≤—ã",
            "flashcards_count": 0,
            "activities": []
        }],
        "milestones": [{"title": "–ò–∑—É—á–∏—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª", "progress_percent": 100, "session": 1, "description": "–û—Å–Ω–æ–≤–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ"}],
        "review_schedule": [1, 3, 7],
        "total_hours": 0.75,
        "completion_date": (datetime.now() + timedelta(days=7)).strftime("%d.%m.%Y"),
        "material_analysis": {
            "overall_difficulty": 1.5,
            "estimated_study_time": {"total_hours": 0.75, "study_time": 30, "review_time": 15, "reading_time": 10}
        },
        "adaptive_features": {"difficulty_adjustment": False, "personalized_pace": False, "progress_tracking": False},
        "success_metrics": {"target_retention": 80, "target_completion": 85, "target_satisfaction": 7}
    }

def assess_content_quality(text: str, topics: List[Dict], summary: str, flashcards: List[Dict]) -> Dict:
    """–û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–∑–¥–∞–≤–∞–µ–º–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞"""
    try:
        # –û—Ü–µ–Ω–∫–∞ –≥–ª—É–±–∏–Ω—ã - –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏ —Ç–µ–º –∏ –∫–∞—á–µ—Å—Ç–≤–∞
        depth_score = 0.0
        
        # –ü—Ä–æ–≤—è–µ–º, –µ—Å—Ç—å –ª–∏ —É —Ç–µ–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è (–∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞)
        meaningful_titles = sum(1 for t in topics if len(t.get('title', '')) < 100 and not t['title'].endswith('...'))
        depth_score += min(0.5, meaningful_titles / len(topics) * 0.5) if topics else 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–æ–¥—Ç–µ–º –∏ –ø—Ä–∏–º–µ—Ä–æ–≤
        topics_with_subtopics = sum(1 for t in topics if len(t.get('subtopics', [])) > 0)
        topics_with_examples = sum(1 for t in topics if len(t.get('examples', [])) > 0)
        depth_score += min(0.25, topics_with_subtopics / len(topics) * 0.25) if topics else 0
        depth_score += min(0.25, topics_with_examples / len(topics) * 0.25) if topics else 0
        
        # –û—Ü–µ–Ω–∫–∞ –æ—Ö–≤–∞—Ç–∞ - –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–Ω—è—Ç–∏—è—Ö
        total_concepts = sum(len(t.get('key_concepts', [])) for t in topics)
        coverage_score = min(1.0, total_concepts / 30)
        
        # –û—Ü–µ–Ω–∫–∞ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç–∏ - –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π
        total_examples = sum(len(t.get('examples', [])) for t in topics)
        practical_flashcards = sum(1 for f in flashcards if f.get('type') in ['application', 'problem'])
        practical_score = min(1.0, (total_examples / 10 * 0.5) + (practical_flashcards / 5 * 0.5))
        
        # –û—Ü–µ–Ω–∫–∞ —è—Å–Ω–æ—Å—Ç–∏ - –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä–µ–∑—é–º–µ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞—Ä—Ç–æ—á–µ–∫
        clarity_score = 0.5  # Base score
        if len(summary) > 100 and '##' in summary:
            clarity_score += 0.3
        if flashcards and all('hint' in f and 'memory_hook' in f for f in flashcards[:5]):
            clarity_score += 0.2
        
        suggestions = []
        if depth_score < 0.7:
            suggestions.append("–£–ª—É—á—à–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º - —Å–µ–π—á–∞—Å –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞ –≤–º–µ—Å—Ç–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π")
        if coverage_score < 0.7:
            suggestions.append("–†–∞—Å—à–∏—Ä–∏—Ç—å –æ—Ö–≤–∞—Ç –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π")
        if practical_score < 0.7:
            suggestions.append("–î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        if clarity_score < 0.7:
            suggestions.append("–£–ª—É—á—à–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —è—Å–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è")
        
        return {
            "depth_score": round(depth_score, 2),
            "coverage_score": round(coverage_score, 2),
            "practical_score": round(practical_score, 2),
            "clarity_score": round(clarity_score, 2),
            "overall_score": round((depth_score + coverage_score + practical_score + clarity_score) / 4, 2),
            "suggestions": suggestions
        }
        
    except Exception as e:
        logger.error(f"Error assessing content quality: {str(e)}")
        return {
            "depth_score": 0.5,
            "coverage_score": 0.5,
            "practical_score": 0.5,
            "clarity_score": 0.5,
            "overall_score": 0.5,
            "suggestions": ["–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ"]
        }

def process_file(filepath: str, filename: str, page_range: str = None) -> Dict[str, Any]:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤—ã–±–æ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü"""
    try:
        logger.info(f"Starting processing for: {filename}")
        if page_range:
            logger.info(f"Page range specified: {page_range}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        file_ext = Path(filename).suffix.lower()
        
        if file_ext == '.pdf':
            text = extract_text_from_pdf_with_pages(filepath, page_range)
            video_data = None
        elif file_ext in ['.mp4', '.mov', '.mkv']:
            video_data = transcribe_video_with_timestamps(filepath)
            text = video_data['full_text']
        else:
            raise ValueError(f"Unsupported file type: {file_ext}")
        
        if not text or len(text.strip()) < 100:
            raise ValueError("Extracted text is too short or empty")
        
        logger.info(f"Extracted {len(text)} characters of text")
        
        try:
            topics_data = extract_topics_with_gpt(text)
            logger.info("Successfully extracted topics with GPT")
        except Exception as e:
            logger.warning(f"Failed to extract topics with GPT: {str(e)}, falling back to local method")
            topics_data = extract_topics_fallback(text)
        
        summary = generate_summary(text)
        flashcards = generate_flashcards(text)
        mind_map = generate_mind_map(text, topics_data.get('main_topics', []))
        study_plan = generate_study_plan(topics_data.get('main_topics', []), flashcards, len(text))
        quality = assess_content_quality(text, topics_data.get('main_topics', []), summary, flashcards)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "topics_data": topics_data,
            "summary": summary,
            "flashcards": flashcards,
            "mind_map": mind_map,
            "study_plan": study_plan,
            "quality_assessment": quality,
            "metadata": {
                "filename": filename,
                "file_type": file_ext,
                "text_length": len(text),
                "processing_date": datetime.now().isoformat()
            }
        }
        
        if video_data:
            result["video_segments"] = video_data.get('segments', [])
            result["key_moments"] = video_data.get('key_moments', [])
        
        logger.info(f"Advanced processing complete. Quality score: {quality['overall_score']}")
        
        return result
        
    except Exception as e:
        logger.error(f"Error in advanced processing: {str(e)}")
        raise

# –î–ª—è —Ç–µ—Å—Ç–∞
if __name__ == "__main__":
    sample_text = """
    –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

    –õ–∏–Ω–µ–π–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —è–≤–ª—è–µ—Ç—Å—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏.
    –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç—å—é.
    
    –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç:
    - –¢–æ—á–Ω–æ—Å—Ç—å (precision) - –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    - –ü–æ–ª–Ω–æ—Ç–∞ (recall) - –¥–æ–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    - F-–º–µ—Ä–∞ - –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç—ã
    
    –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è
    —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.
    
    –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤, —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤, –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞.
    """
    
    print("Testing advanced topic extraction...")
    topics_data = extract_topics_fallback(sample_text)
    print(f"Found {len(topics_data['main_topics'])} main topics")
    if topics_data['main_topics']:
        print(f"First topic: {topics_data['main_topics'][0]['title']}")
        print(f"Summary: {topics_data['main_topics'][0]['summary'][:100]}...")
    
    if os.environ.get('OPENAI_API_KEY'):
        print("\nTesting with GPT...")
        topics_gpt = extract_topics_with_gpt(sample_text)
        if topics_gpt.get('main_topics'):
            print(f"GPT topic: {topics_gpt['main_topics'][0]['title']}")
    else:
        print("\nSkipping GPT tests - OPENAI_API_KEY not set")