import os
import json
import logging
import re
from typing import List, Dict, Tuple, Any
import numpy as np
from pathlib import Path
from dotenv import load_dotenv
from datetime import datetime, timedelta

# Load environment variables
load_dotenv()

# PDF processing
from pdfminer.high_level import extract_text

# Video/Audio processing
import whisperx
import torch

# NLP and embeddings
from sentence_transformers import SentenceTransformer
from sklearn.cluster import HDBSCAN
from sklearn.preprocessing import normalize
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from collections import Counter

# OpenAI for summarization and flashcards
from openai import OpenAI

# Download NLTK data if needed
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Configure logging
logger = logging.getLogger(__name__)

# Initialize models
device = "cuda" if torch.cuda.is_available() else "cpu"
compute_type = "float16" if device == "cuda" else "int8"

# Global variables for models
sentence_model = None
whisper_model = None
openai_client = None

def load_models():
    """Load AI models"""
    global sentence_model, whisper_model, openai_client
    
    logger.info("Loading models...")
    
    # Sentence transformer for embeddings
    sentence_model = SentenceTransformer("intfloat/e5-large-v2", device=device)
    
    # Whisper for audio transcription
    try:
        whisper_model = whisperx.load_model("large-v3", device, compute_type=compute_type)
    except Exception as e:
        logger.warning(f"Whisper model not loaded: {str(e)}")
    
    # OpenAI client
    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        raise ValueError("OPENAI_API_KEY environment variable not set")
    openai_client = OpenAI(api_key=api_key)
    
    logger.info("Models loaded successfully")

# Load models on module import
try:
    load_models()
except Exception as e:
    logger.warning(f"Models not loaded on import: {str(e)}")

def extract_text_from_pdf(filepath: str) -> str:
    """Extract text from PDF file"""
    try:
        text = extract_text(filepath)
        return text.strip()
    except Exception as e:
        logger.error(f"Error extracting text from PDF: {str(e)}")
        raise

def transcribe_video_with_timestamps(filepath: str) -> Dict[str, Any]:
    """Transcribe video/audio with timestamps using Whisper"""
    try:
        logger.info(f"Transcribing video with timestamps: {filepath}")
        
        # Load audio
        audio = whisperx.load_audio(filepath)
        
        # Transcribe with timestamps
        result = whisper_model.transcribe(audio, batch_size=16)
        
        # Process segments
        segments = []
        key_moments = []
        full_text = ""
        
        for i, segment in enumerate(result["segments"]):
            text = segment["text"].strip()
            start = segment["start"]
            end = segment["end"]
            
            full_text += text + " "
            
            # Analyze segment importance (based on length and keywords)
            importance = min(1.0, len(text.split()) / 50)
            
            segments.append({
                "start": start,
                "end": end,
                "text": text,
                "importance": importance
            })
            
            # Identify key moments (longer segments with high importance)
            if importance > 0.7 and len(text.split()) > 20:
                key_moments.append({
                    "time": start,
                    "description": text[:100] + "..." if len(text) > 100 else text
                })
        
        return {
            "full_text": full_text.strip(),
            "segments": segments,
            "key_moments": key_moments[:10]  # Top 10 key moments
        }
        
    except Exception as e:
        logger.error(f"Error transcribing video: {str(e)}")
        # Fallback to simple transcription
        return {"full_text": transcribe_video_simple(filepath), "segments": [], "key_moments": []}

def transcribe_video_simple(filepath: str) -> str:
    """Simple video transcription without timestamps"""
    try:
        audio = whisperx.load_audio(filepath)
        result = whisper_model.transcribe(audio, batch_size=16)
        text = " ".join([segment["text"] for segment in result["segments"]])
        return text.strip()
    except Exception as e:
        logger.error(f"Error in simple transcription: {str(e)}")
        raise

def extract_topics_with_gpt(text: str) -> Dict[str, Any]:
    """Extract topics using GPT-4 for better understanding"""
    try:
        if not openai_client:
            load_models()
        
        # Limit text for API
        max_chars = 15000
        if len(text) > max_chars:
            text = text[:max_chars] + "..."
        
        prompt = """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —É—á–µ–±–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –∏–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

–í–µ—Ä–Ω–∏ JSON –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ:
{
  "main_topics": [
    {
      "title": "–ö—Ä–∞—Ç–∫–æ–µ, –ø–æ–Ω—è—Ç–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã (–Ω–µ –∫–æ–ø–∏—è —Ç–µ–∫—Å—Ç–∞!)",
      "summary": "–û–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ç–µ–º—ã —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
      "subtopics": ["–ü–æ–¥—Ç–µ–º–∞ 1", "–ü–æ–¥—Ç–µ–º–∞ 2"],
      "key_concepts": ["–ö–ª—é—á–µ–≤–æ–µ –ø–æ–Ω—è—Ç–∏–µ 1", "–ö–ª—é—á–µ–≤–æ–µ –ø–æ–Ω—è—Ç–∏–µ 2"],
      "complexity": "basic/intermediate/advanced",
      "examples": ["–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è"],
      "why_important": "–ü–æ—á–µ–º—É —ç—Ç–∞ —Ç–µ–º–∞ –≤–∞–∂–Ω–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞"
    }
  ],
  "concept_map": {
    "relationships": [
      {
        "from": "–¢–µ–º–∞ 1",
        "to": "–¢–µ–º–∞ 2", 
        "type": "causes/requires/similar/contrast/part_of",
        "description": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–≤—è–∑–∏"
      }
    ]
  },
  "learning_objectives": ["–ß—Ç–æ —Å—Ç—É–¥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –ø–æ–Ω—è—Ç—å –ø–æ—Å–ª–µ –∏–∑—É—á–µ–Ω–∏—è"],
  "prerequisites": ["–ß—Ç–æ –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å –¥–æ –∏–∑—É—á–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞"]
}

–í–ê–ñ–ù–û:
1. –ù–ï –∫–æ–ø–∏—Ä—É–π —Ç–µ–∫—Å—Ç –¥–æ—Å–ª–æ–≤–Ω–æ! –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
2. –°–æ–∑–¥–∞–π –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏", –∞ –Ω–µ "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è...")
3. –ò–∑–≤–ª–µ–∫–∏ 5-10 –≥–ª–∞–≤–Ω—ã—Ö —Ç–µ–º
4. –ö–∞–∂–¥–∞—è —Ç–µ–º–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏—á–µ—Å–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–π
5. –ü—Ä–∏–º–µ—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º–∏

–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:"""
        
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. –ò–∑–≤–ª–µ–∫–∞–π –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ç–µ–º—ã, –∞ –Ω–µ –∫–æ–ø–∏—Ä—É–π —Ç–µ–∫—Å—Ç."},
                {"role": "user", "content": f"{prompt}\n\n{text}"}
            ],
            temperature=0.3,
            max_tokens=3000,
            response_format={"type": "json_object"}
        )
        
        topics_data = json.loads(response.choices[0].message.content)
        
        # Ensure all required fields
        for topic in topics_data.get("main_topics", []):
            topic.setdefault("why_important", "–í–∞–∂–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞")
            topic.setdefault("examples", [])
            topic.setdefault("subtopics", [])
            topic.setdefault("key_concepts", [])
        
        return topics_data
        
    except Exception as e:
        logger.error(f"Error extracting topics with GPT: {str(e)}")
        # Fallback to old method
        return extract_topics_fallback(text)

def extract_topics_fallback(text: str) -> Dict[str, Any]:
    """Fallback method for topic extraction without GPT"""
    try:
        # Split into paragraphs instead of sentences for better context
        paragraphs = [p.strip() for p in text.split('\n\n') if len(p.strip()) > 50]
        
        if len(paragraphs) < 3:
            # If not enough paragraphs, split by sentences
            sentences = sent_tokenize(text)
            # Group sentences into pseudo-paragraphs
            paragraphs = []
            for i in range(0, len(sentences), 3):
                paragraph = " ".join(sentences[i:i+3])
                if len(paragraph) > 50:
                    paragraphs.append(paragraph)
        
        if len(paragraphs) < 2:
            return {
                "main_topics": [{
                    "title": "–û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                    "summary": "–î–æ–∫—É–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞",
                    "subtopics": [],
                    "key_concepts": [],
                    "complexity": "basic",
                    "examples": [],
                    "why_important": "–û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"
                }],
                "concept_map": {"relationships": []},
                "learning_objectives": ["–ò–∑—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"],
                "prerequisites": []
            }
        
        # Generate embeddings for paragraphs
        embeddings = sentence_model.encode(paragraphs, convert_to_tensor=False)
        embeddings = normalize(embeddings)
        
        # Hierarchical clustering
        min_cluster_size = max(2, min(5, len(paragraphs) // 5))
        clusterer = HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=1,
            metric='euclidean',
            cluster_selection_method='eom'
        )
        
        cluster_labels = clusterer.fit_predict(embeddings)
        
        # Extract topics from clusters
        main_topics = []
        unique_labels = set(cluster_labels)
        
        for label in unique_labels:
            if label == -1:  # Skip noise
                continue
            
            # Get paragraphs in this cluster
            cluster_paragraphs = [paragraphs[i] for i, l in enumerate(cluster_labels) if l == label]
            
            if not cluster_paragraphs:
                continue
            
            # Extract a meaningful title (not just first sentence)
            title = extract_topic_title(cluster_paragraphs)
            
            # Create a proper summary
            summary = create_topic_summary(cluster_paragraphs)
            
            # Extract key concepts
            key_concepts = extract_key_concepts(" ".join(cluster_paragraphs))
            
            # Determine complexity
            complexity = determine_complexity(" ".join(cluster_paragraphs))
            
            # Find examples
            examples = extract_meaningful_examples(cluster_paragraphs)
            
            # Create topic
            topic = {
                "title": title,
                "summary": summary,
                "subtopics": extract_subtopics_smart(cluster_paragraphs),
                "key_concepts": key_concepts[:5],
                "complexity": complexity,
                "examples": examples[:3],
                "why_important": f"–≠—Ç–∞ —Ç–µ–º–∞ –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å {title.lower()}"
            }
            
            main_topics.append(topic)
        
        # Sort by importance
        main_topics.sort(key=lambda x: len(x["summary"]), reverse=True)
        
        # Extract relationships
        relationships = extract_smart_relationships(main_topics, text)
        
        # Extract learning objectives
        learning_objectives = extract_learning_objectives(main_topics)
        
        return {
            "main_topics": main_topics[:10],
            "concept_map": {"relationships": relationships},
            "learning_objectives": learning_objectives,
            "prerequisites": []
        }
        
    except Exception as e:
        logger.error(f"Error in fallback topic extraction: {str(e)}")
        return {
            "main_topics": [{
                "title": "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞",
                "summary": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–º—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "subtopics": [],
                "key_concepts": [],
                "complexity": "basic",
                "examples": [],
                "why_important": "–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–æ–π –∞–Ω–∞–ª–∏–∑"
            }],
            "concept_map": {"relationships": []},
            "learning_objectives": ["–ò–∑—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç"],
            "prerequisites": []
        }

def extract_topic_title(paragraphs: List[str]) -> str:
    """Extract meaningful topic title from paragraphs"""
    # Look for headers or strong statements
    for para in paragraphs:
        # Check for section headers (usually short and at the beginning)
        lines = para.split('\n')
        for line in lines:
            if len(line) < 100 and (
                line.strip().startswith('¬ß') or 
                line.strip()[0].isdigit() or
                line.isupper() or
                ':' in line[:50]
            ):
                # Clean up the title
                title = line.strip()
                title = re.sub(r'^[¬ß\d\s.]+', '', title)  # Remove section numbers
                title = title.strip(':').strip()
                if len(title) > 10:
                    return title[:80]
    
    # If no header found, extract key phrase
    all_text = " ".join(paragraphs[:2])  # Use first two paragraphs
    
    # Try to find the main concept
    concept_patterns = [
        r'([–ê-–Ø][–∞-—è]+(?:\s+[–∞-—è]+){0,3})\s+(?:‚Äî|—ç—Ç–æ|—è–≤–ª—è–µ—Ç—Å—è|–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç)',
        r'(?:–†–∞—Å—Å–º–æ—Ç—Ä–∏–º|–ò–∑—É—á–∏–º|–û–±—Å—É–¥–∏–º)\s+([–∞-—è]+(?:\s+[–∞-—è]+){0,3})',
        r'(?:–∑–∞–¥–∞—á[–∞–∏]|–º–µ—Ç–æ–¥|–∞–ª–≥–æ—Ä–∏—Ç–º|—Ñ—É–Ω–∫—Ü–∏[—è–∏]|–º–æ–¥–µ–ª[—å–∏])\s+([–∞-—è]+(?:\s+[–∞-—è]+){0,2})'
    ]
    
    for pattern in concept_patterns:
        match = re.search(pattern, all_text, re.IGNORECASE)
        if match:
            title = match.group(1).strip()
            if len(title) > 10:
                return title.capitalize()
    
    # Last resort - use most important noun phrases
    words = word_tokenize(all_text.lower())
    # Find noun phrases (simplified)
    important_words = [w for w in words if len(w) > 4 and w.isalpha()]
    word_freq = Counter(important_words)
    
    if word_freq:
        top_words = [word for word, _ in word_freq.most_common(3)]
        return " ".join(top_words).capitalize()
    
    return "–¢–µ–º–∞ —Ä–∞–∑–¥–µ–ª–∞"

def create_topic_summary(paragraphs: List[str]) -> str:
    """Create meaningful summary from paragraphs"""
    # Don't just concatenate - extract key ideas
    key_sentences = []
    
    for para in paragraphs:
        sentences = sent_tokenize(para)
        for sent in sentences:
            # Look for definitional or explanatory sentences
            if any(marker in sent.lower() for marker in [
                '—ç—Ç–æ', '—è–≤–ª—è–µ—Ç—Å—è', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç', '–æ–∑–Ω–∞—á–∞–µ—Ç',
                '–ø–æ–∑–≤–æ–ª—è–µ—Ç', '–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è', '–ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è',
                '–º–æ–∂–Ω–æ', '—Å–ª–µ–¥—É–µ—Ç', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ'
            ]):
                key_sentences.append(sent)
                if len(key_sentences) >= 2:
                    break
    
    if not key_sentences:
        # Take first meaningful sentence
        for para in paragraphs:
            sentences = sent_tokenize(para)
            for sent in sentences:
                if len(sent) > 30:
                    key_sentences.append(sent)
                    break
            if key_sentences:
                break
    
    # Combine and clean
    summary = " ".join(key_sentences[:2])
    # Remove formulas and special characters
    summary = re.sub(r'\$[^$]+\$', '[—Ñ–æ—Ä–º—É–ª–∞]', summary)
    summary = re.sub(r'[^\w\s\[\].,!?;:()-]', '', summary)
    
    return summary[:300]

def extract_subtopics_smart(paragraphs: List[str]) -> List[str]:
    """Extract meaningful subtopics"""
    subtopics = []
    
    for para in paragraphs:
        # Look for enumeration or listing
        if re.search(r'(?:–≤–∫–ª—é—á–∞–µ—Ç|—Å–æ–¥–µ—Ä–∂–∏—Ç|—Å–æ—Å—Ç–æ–∏—Ç –∏–∑|—Ä–∞–∑–ª–∏—á–∞—é—Ç|–≤—ã–¥–µ–ª—è—é—Ç):', para, re.IGNORECASE):
            # Extract items after colon
            parts = para.split(':')
            if len(parts) > 1:
                items = re.split(r'[;,]', parts[1])
                for item in items[:4]:
                    item = item.strip()
                    if 10 < len(item) < 100:
                        subtopics.append(item)
        
        # Look for numbered items
        numbered = re.findall(r'\d+\)\s*([^.]+)\.', para)
        subtopics.extend([item.strip() for item in numbered[:4] if 10 < len(item) < 100])
    
    # Remove duplicates while preserving order
    seen = set()
    unique_subtopics = []
    for topic in subtopics:
        if topic.lower() not in seen:
            seen.add(topic.lower())
            unique_subtopics.append(topic)
    
    return unique_subtopics[:5]

def extract_meaningful_examples(paragraphs: List[str]) -> List[str]:
    """Extract actual examples, not just random sentences"""
    examples = []
    
    for para in paragraphs:
        sentences = sent_tokenize(para)
        for sent in sentences:
            # Look for example indicators
            if any(indicator in sent.lower() for indicator in [
                '–Ω–∞–ø—Ä–∏–º–µ—Ä', '–∫ –ø—Ä–∏–º–µ—Ä—É', '–≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏', '—Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º',
                '–ø—É—Å—Ç—å', '–¥–æ–ø—É—Å—Ç–∏–º', '–ø—Ä–µ–¥—Å—Ç–∞–≤–∏–º', '–≤–æ–∑—å–º–µ–º'
            ]):
                # Clean up the example
                example = sent.strip()
                # Remove the indicator for cleaner text
                for indicator in ['–ù–∞–ø—Ä–∏–º–µ—Ä,', '–ö –ø—Ä–∏–º–µ—Ä—É,', '–í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏,']:
                    example = example.replace(indicator, '').strip()
                
                if len(example) > 20:
                    examples.append(example)
            
            # Look for concrete values or scenarios
            elif re.search(r'\d+\s*(?:%|–ø—Ä–æ—Ü–µ–Ω—Ç|–æ–±—ä–µ–∫—Ç|—ç–ª–µ–º–µ–Ω—Ç|–∫–ª–∞—Å—Å)', sent) and len(sent) < 200:
                examples.append(sent.strip())
    
    return examples[:5]

def extract_smart_relationships(topics: List[Dict], text: str) -> List[Dict]:
    """Extract meaningful relationships between topics"""
    relationships = []
    
    # Create a mapping of topic keywords
    topic_keywords = {}
    for i, topic in enumerate(topics):
        keywords = set()
        # Add words from title
        keywords.update(word_tokenize(topic['title'].lower()))
        # Add key concepts
        for concept in topic['key_concepts']:
            keywords.update(word_tokenize(concept.lower()))
        topic_keywords[i] = keywords
    
    # Look for relationships in text
    relationship_patterns = {
        'causes': ['–ø—Ä–∏–≤–æ–¥–∏—Ç –∫', '–≤—ã–∑—ã–≤–∞–µ—Ç', '–≤–ª–∏—è–µ—Ç –Ω–∞', '–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç'],
        'requires': ['—Ç—Ä–µ–±—É–µ—Ç', '–Ω–µ–æ–±—Ö–æ–¥–∏–º', '–Ω—É–∂–µ–Ω –¥–ª—è', '–æ—Å–Ω–æ–≤–∞–Ω –Ω–∞'],
        'part_of': ['—á–∞—Å—Ç—å', '–≤–∫–ª—é—á–∞–µ—Ç', '—Å–æ—Å—Ç–æ–∏—Ç –∏–∑', '—Å–æ–¥–µ—Ä–∂–∏—Ç'],
        'contrast': ['–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç', '–Ω–∞–ø—Ä–æ—Ç–∏–≤', '–æ–¥–Ω–∞–∫–æ', '–Ω–æ'],
        'similar': ['–ø–æ—Ö–æ–∂', '–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ', '—Ç–∞–∫–∂–µ –∫–∞–∫', '–ø–æ–¥–æ–±–Ω–æ']
    }
    
    # Analyze text for relationships
    sentences = sent_tokenize(text.lower())
    
    for i, topic1 in enumerate(topics):
        for j, topic2 in enumerate(topics):
            if i >= j:
                continue
            
            # Check if topics are mentioned together
            for sent in sentences:
                t1_found = any(kw in sent for kw in topic_keywords[i])
                t2_found = any(kw in sent for kw in topic_keywords[j])
                
                if t1_found and t2_found:
                    # Check for relationship type
                    for rel_type, patterns in relationship_patterns.items():
                        if any(pattern in sent for pattern in patterns):
                            relationships.append({
                                "from": topic1['title'],
                                "to": topic2['title'],
                                "type": rel_type,
                                "description": f"{topic1['title']} {rel_type} {topic2['title']}"
                            })
                            break
                    
                    # Only one relationship per pair
                    break
    
    return relationships[:10]

def extract_learning_objectives(topics: List[Dict]) -> List[str]:
    """Extract learning objectives from topics"""
    objectives = []
    
    # Common objective patterns
    objective_verbs = [
        "–ü–æ–Ω–∏–º–∞—Ç—å", "–û–±—ä—è—Å–Ω—è—Ç—å", "–ü—Ä–∏–º–µ–Ω—è—Ç—å", "–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å",
        "–†–∞–∑–ª–∏—á–∞—Ç—å", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å", "–í—ã—á–∏—Å–ª—è—Ç—å", "–û–ø—Ä–µ–¥–µ–ª—è—Ç—å"
    ]
    
    for topic in topics[:5]:  # Top 5 topics
        # Create objective based on topic
        verb = objective_verbs[len(objectives) % len(objective_verbs)]
        
        if topic['complexity'] == 'basic':
            objective = f"{verb} –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è {topic['title'].lower()}"
        elif topic['complexity'] == 'intermediate':
            objective = f"{verb} –∏ –ø—Ä–∏–º–µ–Ω—è—Ç—å {topic['title'].lower()}"
        else:
            objective = f"{verb} –∏ —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ —Ç–µ–º–µ {topic['title'].lower()}"
        
        objectives.append(objective)
    
    # Add general objectives
    objectives.extend([
        "–†–µ—à–∞—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –ø–æ –∏–∑—É—á–µ–Ω–Ω—ã–º —Ç–µ–º–∞–º",
        "–°–≤—è–∑—ã–≤–∞—Ç—å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º"
    ])
    
    return objectives[:7]

def extract_key_concepts(text: str) -> List[str]:
    """Extract key concepts - improved version"""
    concepts = []
    
    # Look for defined terms
    defined_terms = re.findall(
        r'(?:([–ê-–Ø][–∞-—è]+(?:\s+[–∞-—è]+){0,2})\s*(?:‚Äî|—ç—Ç–æ|–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è|—è–≤–ª—è–µ—Ç—Å—è))',
        text
    )
    concepts.extend([term.strip() for term in defined_terms if len(term) > 5])
    
    # Look for terms in parentheses (often English terms or abbreviations)
    parenthetical = re.findall(r'\(([A-Za-z]+(?:\s+[A-Za-z]+){0,2})\)', text)
    concepts.extend([term for term in parenthetical if len(term) > 3])
    
    # Look for emphasized terms
    emphasized = re.findall(r'¬´([^¬ª]+)¬ª', text)
    concepts.extend([term for term in emphasized if 5 < len(term) < 50])
    
    # Frequency-based extraction with filtering
    words = word_tokenize(text.lower())
    # Filter for meaningful words
    meaningful_words = [
        w for w in words 
        if len(w) > 4 and w.isalpha() and
        not w in ['–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–±—É–¥–µ—Ç', '–º–æ–∂–µ—Ç', '—ç—Ç–æ–≥–æ', '—ç—Ç–æ–º—É']
    ]
    
    word_freq = Counter(meaningful_words)
    
    # Add frequent terms
    for word, freq in word_freq.most_common(20):
        if freq > 3 and word not in [c.lower() for c in concepts]:
            # Check if it's a meaningful concept
            if any(pattern in text.lower() for pattern in [
                f'{word} —ç—Ç–æ',
                f'{word} —è–≤–ª—è–µ—Ç—Å—è',
                f'{word} –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç',
                f'–∏—Å–ø–æ–ª—å–∑—É—è {word}',
                f'–º–µ—Ç–æ–¥–æ–º {word}'
            ]):
                concepts.append(word.capitalize())
    
    # Remove duplicates while preserving order
    seen = set()
    unique_concepts = []
    for concept in concepts:
        if concept.lower() not in seen:
            seen.add(concept.lower())
            unique_concepts.append(concept)
    
    return unique_concepts[:15]

def determine_complexity(text: str) -> str:
    """Determine text complexity level"""
    words = word_tokenize(text.lower())
    
    # Indicators of complexity
    basic_words = ['–æ—Å–Ω–æ–≤–Ω–æ–π', '–ø—Ä–æ—Å—Ç–æ–π', '–±–∞–∑–æ–≤—ã–π', '—ç–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω—ã–π', '–Ω–∞—á–∞–ª—å–Ω—ã–π']
    intermediate_words = ['–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ', '–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ', '–∞–ª–≥–æ—Ä–∏—Ç–º', '–º–µ—Ç–æ–¥', '–∞–Ω–∞–ª–∏–∑']
    advanced_words = ['–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è', '–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ', '—Ç–µ–æ—Ä–µ–º–∞', '—Å–ª–æ–∂–Ω–æ—Å—Ç—å', '–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è']
    
    basic_count = sum(1 for w in words if w in basic_words)
    intermediate_count = sum(1 for w in words if w in intermediate_words)
    advanced_count = sum(1 for w in words if w in advanced_words)
    
    # Check for mathematical formulas
    formula_count = len(re.findall(r'[‚àë‚à´‚àÇ‚àá‚àà‚àÄ‚àÉ]|\$[^$]+\$', text))
    
    # Sentence complexity
    sentences = sent_tokenize(text)
    avg_sentence_length = np.mean([len(word_tokenize(s)) for s in sentences]) if sentences else 0
    
    # Decision logic
    if advanced_count > 2 or formula_count > 5 or avg_sentence_length > 25:
        return "advanced"
    elif intermediate_count > 2 or formula_count > 2 or avg_sentence_length > 20:
        return "intermediate"
    else:
        return "basic"

def generate_summary(text: str) -> str:
    """Generate structured summary with GPT-4"""
    try:
        if not openai_client:
            load_models()
        
        # Limit text for API
        max_chars = 10000
        if len(text) > max_chars:
            text = text[:max_chars] + "..."
        
        prompt = """–°–æ–∑–¥–∞–π –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–û–ï —Ä–µ–∑—é–º–µ —É—á–µ–±–Ω–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:

## üéØ –ì–ª–∞–≤–Ω–∞—è –∏–¥–µ—è
[1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –æ—Å–Ω–æ–≤–Ω–æ–π –º—ã—Å–ª—å—é –≤—Å–µ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞. –ù–ï –∫–æ–ø–∏—Ä—É–π —Ç–µ–∫—Å—Ç, –∞ –æ–±—ä—è—Å–Ω–∏ —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏]

## üìä –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
‚Ä¢ **–ö–æ–Ω—Ü–µ–ø—Ü–∏—è 1**: –∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ (–ø–æ—á–µ–º—É –≤–∞–∂–Ω–æ)
‚Ä¢ **–ö–æ–Ω—Ü–µ–ø—Ü–∏—è 2**: –∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ (–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ)
[3-5 –∫–æ–Ω—Ü–µ–ø—Ü–∏–π]

## üîó –í–∑–∞–∏–º–æ—Å–≤—è–∑–∏
‚Ä¢ –ö–∞–∫ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è X –≤–ª–∏—è–µ—Ç –Ω–∞ Y
‚Ä¢ –ü–æ—á–µ–º—É Z —è–≤–ª—è–µ—Ç—Å—è —Å–ª–µ–¥—Å—Ç–≤–∏–µ–º X
[2-3 –≤–∞–∂–Ω–µ–π—à–∏–µ —Å–≤—è–∑–∏]

## üí° –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å
‚Ä¢ –ì–¥–µ —ç—Ç–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ–π –∂–∏–∑–Ω–∏
‚Ä¢ –ö–∞–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Ä–µ—à–∞–µ—Ç
[2-3 –ø—Ä–∏–º–µ—Ä–∞]

## ‚ö° –ó–∞–ø–æ–º–Ω–∏—Ç—å –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å
‚Ä¢ –§–∞–∫—Ç 1 (–æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ)
‚Ä¢ –§–∞–∫—Ç 2 (–æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ)
‚Ä¢ –§–∞–∫—Ç 3 (–æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ)

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –ù–ï –ö–û–ü–ò–†–£–ô —Ç–µ–∫—Å—Ç –¥–æ—Å–ª–æ–≤–Ω–æ! –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∏ –æ–±—ä—è—Å–Ω–∏ —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
- –í—ã–¥–µ–ª—è–π –≤–∞–∂–Ω–æ–µ **–∂–∏—Ä–Ω—ã–º**
- –ú–∞–∫—Å–∏–º—É–º 200 —Å–ª–æ–≤ –æ–±—â–∏–π –æ–±—ä–µ–º
- –§–æ–∫—É—Å –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–∏, –∞ –Ω–µ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–∏
- –ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ"""
        
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. –ù–ï –∫–æ–ø–∏—Ä—É–π —Ç–µ–∫—Å—Ç, –∞ –æ–±—ä—è—Å–Ω—è–π —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏."},
                {"role": "user", "content": f"{prompt}\n\n–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{text}"}
            ],
            temperature=0.7,
            max_tokens=800
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        logger.error(f"Error generating advanced summary: {str(e)}")
        return "## üéØ –ì–ª–∞–≤–Ω–∞—è –∏–¥–µ—è\n–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –∏–∑-–∑–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–∏."

def generate_flashcards(text: str) -> List[Dict]:
    """Generate multi-level flashcards with GPT-4"""
    try:
        if not openai_client:
            load_models()
        
        # Limit text for API
        max_chars = 10000
        if len(text) > max_chars:
            text = text[:max_chars] + "..."
        
        prompt = """–°–æ–∑–¥–∞–π –ú–ù–û–ì–û–£–†–û–í–ù–ï–í–´–ï —Ñ–ª–µ—à-–∫–∞—Ä—Ç—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è –î–ê–ù–ù–û–ì–û –ö–û–ù–ö–†–ï–¢–ù–û–ì–û –¢–ï–ö–°–¢–ê.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
1. –í–°–ï –æ—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –¢–û–õ–¨–ö–û –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç - –ø–∏—à–∏ "–í —Ç–µ–∫—Å—Ç–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ" –∏–ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏ –∫–∞—Ä—Ç–æ—á–∫—É
3. –ù–ï –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å–≤–æ–∏—Ö –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π
4. –¶–∏—Ç–∏—Ä—É–π –∏–ª–∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π –¢–û–õ–¨–ö–û —Ç–æ, —á—Ç–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
5. –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å —Ñ–æ—Ä–º—É–ª—ã, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –ø—Ä–∏–º–µ—Ä—ã - –∏—Å–ø–æ–ª—å–∑—É–π –∏–º–µ–Ω–Ω–æ –∏—Ö

–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 15 –∫–∞—Ä—Ç–æ—á–µ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ö–ê–ñ–î–û–ô –∫–∞—Ä—Ç–æ—á–∫–∏:
{
  "type": "—Ç–∏–ø –∫–∞—Ä—Ç–æ—á–∫–∏",
  "q": "–û–°–ú–´–°–õ–ï–ù–ù–´–ô –≤–æ–ø—Ä–æ—Å –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞",
  "a": "–û—Ç–≤–µ—Ç, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –°–¢–†–û–ì–û –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, –Ω–æ —Ç–æ—á–Ω—ã–π)",
  "hint": "–ø–æ–¥—Å–∫–∞–∑–∫–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ —É–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ä–∞–∑–¥–µ–ª",
  "difficulty": —á–∏—Å–ª–æ –æ—Ç 1 –¥–æ 3,
  "related_topics": ["—Ç–µ–º–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ 1", "—Ç–µ–º–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ 2"],
  "memory_hook": "–∞—Å—Å–æ—Ü–∏–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
  "common_mistakes": "—á—Ç–æ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–ø—É—Ç–∞—Ç—å —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ–∫—Å—Ç—É",
  "text_reference": "–∫—Ä–∞—Ç–∫–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –º–µ—Å—Ç–æ –≤ —Ç–µ–∫—Å—Ç–µ, –æ—Ç–∫—É–¥–∞ –≤–∑—è—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"
}

–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º (—Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏):
1. "definition" (3 –∫–∞—Ä—Ç—ã) - –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ò–ó –¢–ï–ö–°–¢–ê, difficulty: 1
   –ü—Ä–∏–º–µ—Ä: "–ö–∞–∫ –≤ —Ç–µ–∫—Å—Ç–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ (accuracy)?"
   –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ñ–æ—Ä–º—É–ª—É/–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞
   
2. "concept" (4 –∫–∞—Ä—Ç—ã) - –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –ò–ó –¢–ï–ö–°–¢–ê, difficulty: 2
   –ü—Ä–∏–º–µ—Ä: "–ü–æ—á–µ–º—É —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ–∫—Å—Ç—É —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª (1.1) –Ω–µ–ª—å–∑—è –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏?"
   –û—Ç–≤–µ—Ç: —Ç–æ, —á—Ç–æ –Ω–∞–ø–∏—Å–∞–Ω–æ –≤ —Ç–µ–∫—Å—Ç–µ –æ–± —ç—Ç–æ–º
   
3. "application" (3 –∫–∞—Ä—Ç—ã) - –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –û–ü–ò–°–ê–ù–û –í –¢–ï–ö–°–¢–ï, difficulty: 2
   –ü—Ä–∏–º–µ—Ä: "–ö–∞–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ?"
   –û—Ç–≤–µ—Ç: —Ç–æ–ª—å–∫–æ —Ç–µ –ø—Ä–∏–º–µ—Ä—ã, —á—Ç–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
   
4. "comparison" (3 –∫–∞—Ä—Ç—ã) - —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –£–ü–û–ú–Ø–ù–£–¢–´–ï –í –¢–ï–ö–°–¢–ï, difficulty: 3
   –ü—Ä–∏–º–µ—Ä: "–ö–∞–∫ —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ–∫—Å—Ç—É —Å–≤—è–∑–∞–Ω—ã —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–Ω–æ—Ç–∞?"
   –û—Ç–≤–µ—Ç: —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —Å–∫–∞–∑–∞–Ω–æ –≤ —Ç–µ–∫—Å—Ç–µ
   
5. "problem" (2 –∫–∞—Ä—Ç—ã) - –∑–∞–¥–∞—á–∏/–ø—Ä–∏–º–µ—Ä—ã –ò–ó –¢–ï–ö–°–¢–ê, difficulty: 3
   –ü—Ä–∏–º–µ—Ä: "–†–µ—à–∏—Ç–µ –ø—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞: [–µ—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä]"

–ü–†–û–í–ï–†–ö–ê: –ü–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ —É–±–µ–¥–∏—Å—å, —á—Ç–æ:
- –í–æ–ø—Ä–æ—Å –∫–∞—Å–∞–µ—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ï–°–¢–¨ –≤ —Ç–µ–∫—Å—Ç–µ
- –û—Ç–≤–µ—Ç –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –∏–ª–∏ –≤—ã–≤–µ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
- –¢—ã –Ω–µ –¥–æ–±–∞–≤–ª—è–µ—à—å —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è –æ —Ç–µ–º–µ

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –º–∞—Å—Å–∏–≤."""

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "–¢—ã —Å–æ–∑–¥–∞–µ—à—å —Ñ–ª–µ—à-–∫–∞—Ä—Ç—ã –°–¢–†–û–ì–û –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π —Å–≤–æ–∏ –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –æ —Ç–µ–º–µ. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ - –Ω–µ —Å–æ–∑–¥–∞–≤–∞–π –∫–∞—Ä—Ç–æ—á–∫—É. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–º JSON."},
                {"role": "user", "content": f"{prompt}\n\n–¢–µ–∫—Å—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—Ä—Ç–æ—á–µ–∫:\n{text}"}
            ],
            temperature=0.3,  # –°–Ω–∏–∑–∏–ª —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç—É
            max_tokens=3000
        )
        
        content = response.choices[0].message.content.strip()
        
        # Extract JSON
        json_match = re.search(r'\[.*\]', content, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
        else:
            json_str = content
        
        # Parse and validate
        flashcards = json.loads(json_str)
        
        # Validate that cards are based on text content
        validated_cards = []
        for card in flashcards:
            # Ensure required fields
            if 'q' in card and 'a' in card:
                # Add text_reference if not present
                if 'text_reference' not in card:
                    card['text_reference'] = "–°–º. —Ç–µ–∫—Å—Ç –≤—ã—à–µ"
                
                # Add spaced repetition data
                card['next_review'] = calculate_next_review(card.get('difficulty', 2))
                card['ease_factor'] = 2.5
                
                validated_cards.append(card)
        
        return validated_cards if validated_cards else generate_fallback_flashcards(text)
        
    except Exception as e:
        logger.error(f"Error generating advanced flashcards: {str(e)}")
        return generate_fallback_flashcards(text)

def generate_fallback_flashcards(text: str) -> List[Dict]:
    """Generate flashcards using rule-based approach when GPT fails"""
    flashcards = []
    
    try:
        # Extract definitions from text
        definition_patterns = [
            r'([–ê-–Ø][–∞-—è]+(?:\s+[–∞-—è]+){0,3})\s+(?:‚Äî|‚Äì|-|—ç—Ç–æ|—è–≤–ª—è–µ—Ç—Å—è)\s*([^.]+)\.',
            r'([–ê-–Ø][–∞-—è]+(?:\s+[–∞-—è]+){0,3})\s+–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è\s+([^.]+)\.',
            r'–ü–æ–¥\s+([–∞-—è]+(?:\s+[–∞-—è]+){0,3})\s+–ø–æ–Ω–∏–º–∞—é—Ç\s+([^.]+)\.'
        ]
        
        for pattern in definition_patterns:
            matches = re.findall(pattern, text, re.MULTILINE)
            for term, definition in matches[:3]:  # Limit definitions
                if len(term) > 3 and len(definition) > 10:
                    flashcards.append({
                        "type": "definition",
                        "q": f"–ß—Ç–æ —Ç–∞–∫–æ–µ {term.strip()}?",
                        "a": f"{term} ‚Äî —ç—Ç–æ {definition.strip()}",
                        "hint": f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–æ –≤ —Ç–µ–∫—Å—Ç–µ",
                        "difficulty": 1,
                        "related_topics": [term.strip()],
                        "memory_hook": f"–ó–∞–ø–æ–º–Ω–∏: {term} = {definition.split()[0]}...",
                        "common_mistakes": "–ù–µ –ø—É—Ç–∞—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏",
                        "text_reference": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
                        "next_review": calculate_next_review(1),
                        "ease_factor": 2.5
                    })
        
        # Extract formulas
        formula_matches = re.findall(r'([A-Za-z]+\([^)]+\))\s*=\s*([^.]+)', text)
        for formula_name, formula_body in formula_matches[:2]:
            flashcards.append({
                "type": "concept",
                "q": f"–ö–∞–∫ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è {formula_name}?",
                "a": f"{formula_name} = {formula_body.strip()}",
                "hint": "–§–æ—Ä–º—É–ª–∞ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ",
                "difficulty": 2,
                "related_topics": ["—Ñ–æ—Ä–º—É–ª—ã", "–≤—ã—á–∏—Å–ª–µ–Ω–∏—è"],
                "memory_hook": f"–§–æ—Ä–º—É–ª–∞: {formula_name}",
                "common_mistakes": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Å–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º—É–ª–µ",
                "text_reference": "–§–æ—Ä–º—É–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
                "next_review": calculate_next_review(2),
                "ease_factor": 2.5
            })
        
        # Extract examples
        example_sentences = re.findall(r'(?:–ù–∞–ø—Ä–∏–º–µ—Ä|–ö –ø—Ä–∏–º–µ—Ä—É|–ü—Ä–∏–º–µ—Ä)[,:]?\s*([^.]+)\.', text, re.IGNORECASE)
        for i, example in enumerate(example_sentences[:2]):
            flashcards.append({
                "type": "application",
                "q": f"–ü—Ä–∏–≤–µ–¥–∏—Ç–µ –ø—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞ #{i+1}",
                "a": example.strip(),
                "hint": "–ü—Ä–∏–º–µ—Ä —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –≤ —Ç–µ–∫—Å—Ç–µ",
                "difficulty": 2,
                "related_topics": ["–ø—Ä–∏–º–µ—Ä—ã", "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ"],
                "memory_hook": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞",
                "common_mistakes": "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø—Ä–∏–º–µ—Ä –ø–æ–ª–Ω—ã–π",
                "text_reference": "–ü—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞",
                "next_review": calculate_next_review(2),
                "ease_factor": 2.5
            })
        
        # Extract key statements
        key_statements = re.findall(r'(?:–í–∞–∂–Ω–æ|–°–ª–µ–¥—É–µ—Ç|–ù–µ–æ–±—Ö–æ–¥–∏–º–æ|–ù—É–∂–Ω–æ)\s+([^.]+)\.', text, re.IGNORECASE)
        for statement in key_statements[:2]:
            flashcards.append({
                "type": "concept",
                "q": f"–ß—Ç–æ –≤–∞–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ–∫—Å—Ç—É?",
                "a": statement.strip(),
                "hint": "–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è",
                "difficulty": 2,
                "related_topics": ["–∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã"],
                "memory_hook": "–í–∞–∂–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ",
                "common_mistakes": "–ù–µ —É–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç",
                "text_reference": "–í–∞–∂–Ω–æ–µ –∑–∞–º–µ—á–∞–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
                "next_review": calculate_next_review(2),
                "ease_factor": 2.5
            })
        
        # If not enough cards, add general ones
        if len(flashcards) < 5:
            sentences = sent_tokenize(text)
            informative_sentences = [s for s in sentences if len(s) > 50 and not s.endswith('?')]
            
            for i, sent in enumerate(informative_sentences[:5-len(flashcards)]):
                flashcards.append({
                    "type": "concept",
                    "q": f"–û–±—ä—è—Å–Ω–∏—Ç–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞: {sent[:50]}...?",
                    "a": sent.strip(),
                    "hint": "–û—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ —Å–∞–º–æ–º –≤–æ–ø—Ä–æ—Å–µ",
                    "difficulty": 1,
                    "related_topics": ["–æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è"],
                    "memory_hook": "–ö–ª—é—á–µ–≤–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ",
                    "common_mistakes": "–ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ",
                    "text_reference": "–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
                    "next_review": calculate_next_review(1),
                    "ease_factor": 2.5
                })
        
        return flashcards
        
    except Exception as e:
        logger.error(f"Error in fallback flashcard generation: {str(e)}")
        # Return at least one card
        return [{
            "type": "definition",
            "q": "–û —á–µ–º —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç?",
            "a": "–¢–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —É—á–µ–±–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è. –ü—Ä–æ—á–∏—Ç–∞–π—Ç–µ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π.",
            "hint": "–ü–æ–¥—É–º–∞–π—Ç–µ –æ –≥–ª–∞–≤–Ω–æ–π —Ç–µ–º–µ",
            "difficulty": 1,
            "related_topics": ["–æ—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞"],
            "memory_hook": "–ì–ª–∞–≤–Ω–∞—è –∏–¥–µ—è —Ç–µ–∫—Å—Ç–∞",
            "common_mistakes": "–ù–µ –ø—É—Ç–∞—Ç—å –¥–µ—Ç–∞–ª–∏ —Å –≥–ª–∞–≤–Ω–æ–π –∏–¥–µ–µ–π",
            "text_reference": "–í–µ—Å—å —Ç–µ–∫—Å—Ç",
            "next_review": calculate_next_review(1),
            "ease_factor": 2.5
        }]

def calculate_next_review(difficulty: int) -> str:
    """Calculate next review date based on difficulty"""
    days_map = {1: 1, 2: 3, 3: 7}
    days = days_map.get(difficulty, 3)
    next_date = datetime.now() + timedelta(days=days)
    return next_date.strftime("%Y-%m-%d")

def generate_mind_map(text: str, topics: List[Dict]) -> Dict:
    """Generate mind map structure"""
    try:
        # Use actual topic titles
        if topics and 'title' in topics[0]:
            central_topic = topics[0]['title']
        else:
            central_topic = "–û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞"
        
        # Create branches from main topics
        branches = []
        colors = ["#FF6B6B", "#4ECDC4", "#45B7D1", "#96CEB4", "#FECA57", "#48DBFB"]
        
        for i, topic in enumerate(topics[:6]):  # Max 6 main branches
            branch = {
                "name": topic['title'][:40],
                "importance": 0.9 - (i * 0.1),
                "color": colors[i % len(colors)],
                "children": []
            }
            
            # Add subtopics as children
            for subtopic in topic.get('subtopics', [])[:3]:
                branch['children'].append({
                    "name": subtopic[:30],
                    "importance": 0.5,
                    "color": branch['color']
                })
            
            # Add key concepts as additional children if no subtopics
            if not branch['children'] and topic.get('key_concepts'):
                for concept in topic['key_concepts'][:3]:
                    branch['children'].append({
                        "name": concept[:30],
                        "importance": 0.4,
                        "color": branch['color']
                    })
            
            branches.append(branch)
        
        return {
            "central_topic": central_topic,
            "branches": branches
        }
        
    except Exception as e:
        logger.error(f"Error generating mind map: {str(e)}")
        return {
            "central_topic": "–¢–µ–º–∞",
            "branches": []
        }

def generate_study_plan(topics: List[Dict], flashcards: List[Dict], estimated_hours: int = 10) -> Dict:
    """Generate personalized study plan with meaningful topics"""
    try:
        # Calculate sessions based on content
        total_cards = len(flashcards)
        total_topics = len(topics)
        
        # Estimate time per session
        minutes_per_session = 45
        total_sessions = max(3, min(10, estimated_hours * 60 // minutes_per_session))
        
        sessions = []
        
        # Group topics by complexity
        basic_topics = [t for t in topics if t.get('complexity') == 'basic']
        intermediate_topics = [t for t in topics if t.get('complexity') == 'intermediate']
        advanced_topics = [t for t in topics if t.get('complexity') == 'advanced']
        
        # Distribute topics across sessions logically
        topic_distribution = []
        
        # First third: basic topics
        for i in range(total_sessions // 3):
            if basic_topics:
                topic_distribution.append(basic_topics.pop(0))
            elif intermediate_topics:
                topic_distribution.append(intermediate_topics.pop(0))
        
        # Second third: intermediate topics
        for i in range(total_sessions // 3, 2 * total_sessions // 3):
            if intermediate_topics:
                topic_distribution.append(intermediate_topics.pop(0))
            elif basic_topics:
                topic_distribution.append(basic_topics.pop(0))
            elif advanced_topics:
                topic_distribution.append(advanced_topics.pop(0))
        
        # Last third: advanced topics and review
        for i in range(2 * total_sessions // 3, total_sessions):
            if advanced_topics:
                topic_distribution.append(advanced_topics.pop(0))
            elif intermediate_topics:
                topic_distribution.append(intermediate_topics.pop(0))
            else:
                # Review previous topics
                if i - 3 >= 0 and i - 3 < len(topic_distribution):
                    topic_distribution.append(topic_distribution[i - 3])
        
        # Create sessions
        cards_per_session = max(3, total_cards // total_sessions)
        card_idx = 0
        
        for day in range(1, total_sessions + 1):
            session_topics = []
            session_cards = []
            
            # Add topic for this session
            if day - 1 < len(topic_distribution):
                topic = topic_distribution[day - 1]
                session_topics.append(topic['title'])
            
            # Add flashcards
            session_card_count = min(cards_per_session, total_cards - card_idx)
            for _ in range(session_card_count):
                if card_idx < total_cards:
                    session_cards.append(card_idx)
                    card_idx += 1
            
            # Determine focus based on day
            if day <= total_sessions // 3:
                focus = "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π"
            elif day <= 2 * total_sessions // 3:
                focus = "–£–≥–ª—É–±–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –∏ –ø—Ä–∞–∫—Ç–∏–∫–∞"
            else:
                focus = "–ó–∞–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è"
            
            # Create meaningful exercises
            exercises = []
            if session_topics:
                topic_name = session_topics[0]
                exercises.extend([
                    f"–û–±—ä—è—Å–Ω–∏—Ç–µ —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏: {topic_name}",
                    f"–ü—Ä–∏–≤–µ–¥–∏—Ç–µ 3 –ø—Ä–∏–º–µ—Ä–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∏–∑ —Ç–µ–º—ã '{topic_name}'",
                    f"–°–æ–∑–¥–∞–π—Ç–µ —Å—Ö–µ–º—É –∏–ª–∏ –¥–∏–∞–≥—Ä–∞–º–º—É –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ '{topic_name}'"
                ])
            
            sessions.append({
                "day": day,
                "topics": session_topics,
                "flashcard_ids": session_cards,
                "duration_minutes": minutes_per_session,
                "focus": focus,
                "exercises": exercises[:3]
            })
        
        # Create meaningful milestones based on actual topics
        milestones = []
        if topics:
            milestones.extend([
                f"–ü–æ–Ω–∏–º–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏: {', '.join([t['title'] for t in topics[:3]])}",
                "–£–º–µ—Ç—å –æ–±—ä—è—Å–Ω–∏—Ç—å –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –º–µ–∂–¥—É –∏–∑—É—á–µ–Ω–Ω—ã–º–∏ —Ç–µ–º–∞–º–∏",
                "–ü—Ä–∏–º–µ–Ω—è—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á",
                f"–£—Å–ø–µ—à–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ 80% —Ñ–ª–µ—à-–∫–∞—Ä—Ç —É—Ä–æ–≤–Ω—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ 2 –∏ –≤—ã—à–µ"
            ])
        
        # Spaced repetition schedule
        review_schedule = [1, 3, 7, 14, 30]  # Days for review
        
        return {
            "sessions": sessions,
            "milestones": milestones[:4],
            "review_schedule": review_schedule,
            "total_hours": total_sessions * minutes_per_session / 60,
            "completion_date": (datetime.now() + timedelta(days=total_sessions)).strftime("%Y-%m-%d")
        }
        
    except Exception as e:
        logger.error(f"Error generating study plan: {str(e)}")
        return {
            "sessions": [{"day": 1, "topics": ["–ò–∑—É—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞"], "duration_minutes": 45}],
            "milestones": ["–ò–∑—É—á–∏—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª"],
            "review_schedule": [1, 3, 7],
            "total_hours": 0.75,
            "completion_date": datetime.now().strftime("%Y-%m-%d")
        }

def assess_content_quality(text: str, topics: List[Dict], summary: str, flashcards: List[Dict]) -> Dict:
    """Assess the quality of generated content"""
    try:
        # Depth score - based on topic hierarchy and quality
        depth_score = 0.0
        
        # Check if topics have meaningful titles (not just text snippets)
        meaningful_titles = sum(1 for t in topics if len(t.get('title', '')) < 100 and not t['title'].endswith('...'))
        depth_score += min(0.5, meaningful_titles / len(topics) * 0.5) if topics else 0
        
        # Check for subtopics and examples
        topics_with_subtopics = sum(1 for t in topics if len(t.get('subtopics', [])) > 0)
        topics_with_examples = sum(1 for t in topics if len(t.get('examples', [])) > 0)
        depth_score += min(0.25, topics_with_subtopics / len(topics) * 0.25) if topics else 0
        depth_score += min(0.25, topics_with_examples / len(topics) * 0.25) if topics else 0
        
        # Coverage score - based on key concepts extracted
        total_concepts = sum(len(t.get('key_concepts', [])) for t in topics)
        coverage_score = min(1.0, total_concepts / 30)
        
        # Practical score - based on examples and applications
        total_examples = sum(len(t.get('examples', [])) for t in topics)
        practical_flashcards = sum(1 for f in flashcards if f.get('type') in ['application', 'problem'])
        practical_score = min(1.0, (total_examples / 10 * 0.5) + (practical_flashcards / 5 * 0.5))
        
        # Clarity score - based on summary structure and flashcard quality
        clarity_score = 0.5  # Base score
        if len(summary) > 100 and '##' in summary:
            clarity_score += 0.3
        if flashcards and all('hint' in f and 'memory_hook' in f for f in flashcards[:5]):
            clarity_score += 0.2
        
        # Generate suggestions
        suggestions = []
        if depth_score < 0.7:
            suggestions.append("–£–ª—É—á—à–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º - —Å–µ–π—á–∞—Å –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞ –≤–º–µ—Å—Ç–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π")
        if coverage_score < 0.7:
            suggestions.append("–†–∞—Å—à–∏—Ä–∏—Ç—å –æ—Ö–≤–∞—Ç –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π")
        if practical_score < 0.7:
            suggestions.append("–î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        if clarity_score < 0.7:
            suggestions.append("–£–ª—É—á—à–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —è—Å–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è")
        
        return {
            "depth_score": round(depth_score, 2),
            "coverage_score": round(coverage_score, 2),
            "practical_score": round(practical_score, 2),
            "clarity_score": round(clarity_score, 2),
            "overall_score": round((depth_score + coverage_score + practical_score + clarity_score) / 4, 2),
            "suggestions": suggestions
        }
        
    except Exception as e:
        logger.error(f"Error assessing content quality: {str(e)}")
        return {
            "depth_score": 0.5,
            "coverage_score": 0.5,
            "practical_score": 0.5,
            "clarity_score": 0.5,
            "overall_score": 0.5,
            "suggestions": ["–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ"]
        }

def process_file(filepath: str, filename: str) -> Dict[str, Any]:
    """Advanced processing pipeline with improved topic extraction"""
    try:
        logger.info(f"Starting advanced processing for: {filename}")
        
        # Extract text based on file type
        file_ext = Path(filename).suffix.lower()
        
        if file_ext == '.pdf':
            text = extract_text_from_pdf(filepath)
            video_data = None
        elif file_ext in ['.mp4', '.mov', '.mkv']:
            video_data = transcribe_video_with_timestamps(filepath)
            text = video_data['full_text']
        else:
            raise ValueError(f"Unsupported file type: {file_ext}")
        
        if not text or len(text.strip()) < 100:
            raise ValueError("Extracted text is too short or empty")
        
        logger.info(f"Extracted {len(text)} characters of text")
        
        # Try to use GPT for better topic extraction
        try:
            topics_data = extract_topics_with_gpt(text)
            logger.info("Successfully extracted topics with GPT")
        except Exception as e:
            logger.warning(f"Failed to extract topics with GPT: {str(e)}, falling back to local method")
            topics_data = extract_topics_fallback(text)
        
        # Generate other components
        summary = generate_summary(text)
        flashcards = generate_flashcards(text)
        mind_map = generate_mind_map(text, topics_data.get('main_topics', []))
        study_plan = generate_study_plan(topics_data.get('main_topics', []), flashcards)
        quality = assess_content_quality(text, topics_data.get('main_topics', []), summary, flashcards)
        
        # Compile results
        result = {
            "topics_data": topics_data,
            "summary": summary,
            "flashcards": flashcards,
            "mind_map": mind_map,
            "study_plan": study_plan,
            "quality_assessment": quality,
            "metadata": {
                "filename": filename,
                "file_type": file_ext,
                "text_length": len(text),
                "processing_date": datetime.now().isoformat()
            }
        }
        
        # Add video-specific data if available
        if video_data:
            result["video_segments"] = video_data.get('segments', [])
            result["key_moments"] = video_data.get('key_moments', [])
        
        logger.info(f"Advanced processing complete. Quality score: {quality['overall_score']}")
        
        return result
        
    except Exception as e:
        logger.error(f"Error in advanced processing: {str(e)}")
        raise

# Test function
if __name__ == "__main__":
    # Test with sample text
    sample_text = """
    –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

    –õ–∏–Ω–µ–π–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —è–≤–ª—è–µ—Ç—Å—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏.
    –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç—å—é.
    
    –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–∫–ª—é—á–∞—é—Ç:
    - –¢–æ—á–Ω–æ—Å—Ç—å (precision) - –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    - –ü–æ–ª–Ω–æ—Ç–∞ (recall) - –¥–æ–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    - F-–º–µ—Ä–∞ - –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç—ã
    
    –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è
    —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.
    
    –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤, —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤, –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞.
    """
    
    print("Testing advanced topic extraction...")
    topics_data = extract_topics_fallback(sample_text)
    print(f"Found {len(topics_data['main_topics'])} main topics")
    if topics_data['main_topics']:
        print(f"First topic: {topics_data['main_topics'][0]['title']}")
        print(f"Summary: {topics_data['main_topics'][0]['summary'][:100]}...")
    
    if os.environ.get('OPENAI_API_KEY'):
        print("\nTesting with GPT...")
        topics_gpt = extract_topics_with_gpt(sample_text)
        if topics_gpt.get('main_topics'):
            print(f"GPT topic: {topics_gpt['main_topics'][0]['title']}")
    else:
        print("\nSkipping GPT tests - OPENAI_API_KEY not set")